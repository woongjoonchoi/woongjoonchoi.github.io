---
title : "Batch Norm in Neural Network Training and Test"



excerpt: "Practical batch norm in neural net"

categories:
  - Optimization
tags:
  - [Machine Learning,batch size,deep learning]
# classes : wide
toc: true
toc_sticky: true
---

## Batch Norm in Neural Network Training and Test

ì´ë²ˆ ê¸€ì—ì„œëŠ” ì‹¤ì œë¡œ Neural Network Training ê³¼ Testì—ì„œ Batch Normì„ ì–´ë–»ê²Œ ì ìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.



## Adding Batch Norm into Nueral Network

$$ X^{[1]}   \implies W^{[1]} , b^{[1]} \implies Z^{[1]} ,  a^{[1]} =  g(Z^{[1]}) \implies W^{[2]}  ,b^{[2]}  \implies Z^{[2]} , a^{[2]} = g(Z{[2]}) \implies  ... $$ 

[]ê°€ hidden layerë¥¼ í‘œí˜„í•˜ëŠ” notationì´ê¸´ í•œë° , ì•ì˜ postë¥¼ ì½ìœ¼ì…§ë‹¤ë©´ ì´í•´ê°€ ë ê±°ë¼ ìƒê°í•©ë‹ˆë‹¤.ğŸ˜€

ì´ê²Œ ì „í˜•ì ì¸ Neural Netì˜ feedforward ì…ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ batch normì„ ì¶”ê°€ì‹œí‚¤ë©´ ì–´ë–»ê²Œ ë ê¹Œìš” ??



$$ Z^{[1]} ,  \mu =  \frac {\Sigma z^{[2]}}{m}, \sigma = \frac{(\Sigma z^{[2]} - \mu )^2}{\sqrt {\mu}}  \implies {z_{norm}}^{[1]} = \frac {(\Sigma z^{[1]}  -  \mu )}{\sqrt {\sigma}} \implies  $$

$$ {z_{norm}}^{[1]} = \frac {(\Sigma z^{[1]} - \mu)} {\sqrt {\sigma^2 + \epsilon }} \implies \widehat{z} = \alpha z_{norm} + \beta  $$    



ìœ„ì™€ ê°™ì´ ìƒˆë¡œìš´ learnable parameters $$\alpha , \beta $$ê°€ ì¶”ê°€ê°€ ë©ë‹ˆë‹¤.  ê¸°ì¡´ì—ëŠ” $$W^{[1]} ,b^{[1]} ,W^{[2]} , b^{[2]}  ..$$ ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ , ì—¬ê¸°ì— $$ {\alpha}^{[1]}  {\beta}^{[1]} , {\alpha}^{[2]} , {\beta}^{[2]} ..$$ ë“¤ì´ ì¶”ê°€ê°€ ë©ë‹ˆë‹¤. 

## Learnable parameters in Neural Network

ì—¬ê¸°ì„œ , ìƒê°í•´ë´ì•¼ í•  ê²ƒì€ ì´ ëª¨ë“  parametersë“¤ì´ í•„ìš”í•œê°€ ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ìˆ˜ì‹ì„ ë´…ì‹œë‹¤.

 $$ Z_{norm} = \frac {(\Sigma z - \mu)} {\sqrt {\sigma^2 + \epsilon}} $$ ëŠ” znormì„ êµ¬í•˜ëŠ” ìˆ˜ì‹ì…ë‹ˆë‹¤. znormì„ êµ¬í•  ë•Œ í‰ê· ì„ ë¹¼ ì£¼ëŠ”ë° , $$ \mu = \frac {\Sigma z }{m}  = \frac{\Sigma {(wx^T + b)} }{m} \implies Z_{norm} = \frac {\Sigma {wx^T}}{\sqrt {\sigma^2 + \epsilon}}$$ ì…ë‹ˆë‹¤.   ì¦‰, bias termì€ ìˆìœ¼ë‚˜ ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ , ì´ bias termì„ ì§€ì›Œì£¼ì–´ë„ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.



batch Normì„ ì‚¬ìš©í•œ training pseudo ì½”ë“œë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

```python
for epoch = 1 ... epochs :
	for  i=1 ... mini-batch :
		feedforward , not using bias term
        	(in each hidden layer replace z_norm with z_hat using alpha, beta )
        calculate db , dw ,da 
        b ,w ,a update using opitmization algorithm(e.g. Adam, RMSProp , SGD ,GD etc)
```





## Batch Norm at Test time

Batch Normì˜ ì´ë¡ ì„ ì´í•´í•˜ì…¨ë‹¤ë©´ ë§¤ë²ˆ batchì˜ mean, varianceë¥¼ êµ¬í•´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, Test timeì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? 1ê°œì˜ inputì´ ë“¤ì–´ì˜¤ëŠ”ë°, ì´ì— ëŒ€í•œ mean, varianceë¥¼ êµ¬í•´ì•¼ í• ê¹Œìš”? ì´ê±´ ë§ì´ ì•ˆë˜ëŠ” ê²ƒì„ ì•Œê³  ìˆìœ¼ì‹¤ ê²ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ neural networks ì—ì„œëŠ” . ê¸°ì¡´ì˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ í›ˆë ¨ì„ í•  ë•Œ $$ \mu , \sigma $$ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ exponentially weighted averageë¥¼ ì´ìš©í•´ì„œ ì¶”ì •ì„ í•˜ê²Œë©ë‹ˆë‹¤.  ì´ ë•Œ , $$ \mu , \sigma $$ëŠ” ê° hidden layerì˜ ê°’ì„ ì´ìš©í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  , ì´ë¥¼ exponentially weighted average ë¥¼ ì´ìš©í•´ì„œ ë§¤ë²ˆ $$ \mu , \sigma $$ ë¥¼ updateë¥¼ í•´ì¤ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ , ëŒ€ëµì ì¸ $$\mu , \sigma$$ ì˜ ê°’ì„ ì•Œì•„ë‚´ê²Œ ë©ë‹ˆë‹¤.   ë‹¤ì‹œ ë§í•˜ìë©´, trainì—ëŠ” mini-batchë¥¼ í†µí•´ì„œ mean, varianceë¥¼ êµ¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, testì—ì„œëŠ” 1ê°œë§Œì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆê³ , single exampleì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ mean, varianceì˜ estimateë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.  ***<u>ì´ëŠ” ì •í™•í•œ mean, varianceë¥¼ êµ¬í•˜ì§€ëŠ” ì•Šì§€ë§Œ, ì •í™•í•˜ê²Œ êµ¬í•˜ëŠ” ê²ƒì— ë¹„í•˜ì—¬ ê½¤ robust í•©ë‹ˆë‹¤.</u>*** í•˜ì§€ë§Œ, ì‹¤ì œë¡œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›ì—ì„œëŠ” ì´ëŸ¬í•œ ê³„ì‚°ì´ ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ êµ¬í˜„ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì–´ì°Œ ëë“  ì˜ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤