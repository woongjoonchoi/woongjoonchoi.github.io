---
title : "Huggingface tutorial: Tokenizer summary"



excerpt: "Tokenizer summary"

categories:
  - Huggingface
tags:
  - [Machine Learning,Huggingface,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---

## Huggingface tutorial ì‹œë¦¬ì¦ˆ : tokenizer

Huggingface tutorial ì‹œë¦¬ì¦ˆì¤‘ tokenizer í¸ì„ ë“£ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)

## What is tokenizer

tokenizerë€ sentenceë¥¼ sub-word í˜¹ì€ word ë‹¨ìœ„ë¡œ ìª¼ê° í›„ ì´ë¥¼ look-up-tableì„ í†µí•´ input idsë¡œ ë³€í™˜í•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. Huggingface tutorialì—ì„œëŠ” íŠ¹íˆ transfomers ê¸°ë°˜ì˜ ëª¨ë¸ë“¤ì— ì‚¬ìš©ë˜ëŠ” tokenizerë¥¼ ì‚´í´ë³´ê²Œ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144427223-af53b57a-f908-4b5c-b584-a425113a9cde.png)



## word-based tokenizer

word-level ë‹¨ìœ„ë¡œ tokenizingí•˜ëŠ” tokenizerëŠ” ì—¬ëŸ¬ê°œê°€ ìˆìŠµë‹ˆë‹¤.

spaceë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizing í•˜ëŠ” tokenizer ì…ë‹ˆë‹¤. spaceë¥¼ ê¸°ì¤€ìœ¼ë¡œ splití•˜ê²Œ ë˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144427638-cb31e5ee-6438-484c-bb21-027ea69988e5.png)

ë¬¸ì¥ë¶€í˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizing í•˜ëŠ” ruleì„ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144427823-345b4540-0a7b-410b-9146-5179cf3f0a96.png)



ì´ë¥¼ look-up-tableì—ì„œ input_ids ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/50165842/144427467-50990986-433e-4005-a1eb-70126e6d0e0e.png)

word-basedì˜ ë¬¸ì œì ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

very large vocabulary size



![image](https://user-images.githubusercontent.com/50165842/144538921-1a218243-073e-4f1c-bcc2-aa80e0f3195c.png)

dog, dogsë¼ëŠ” ë‹¨ì–´ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ë‘ ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ì§€ë‹™ë‹ˆë‹¤. í•˜ì§€ë§Œ, split based tokenizerë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê°ê° ë‹¤ë¥¸ idsë¥¼ ê°€ì§„ ë‹¨ì–´ë¡œ look-up tableì— mappingì´ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ì—„ì²­ë‚œ í¬ê¸°ì˜ vocabularyê°€ ìƒê¸°ê²Œ ë©ë‹ˆë‹¤. ì´ ë§ì€ ë‹¨ì–´ë¥¼ embedding í•˜ê¸° ìœ„í•´ì„œ ë” ë§ì€ parameters ë“¤ì´ í•„ìš”í•˜ê²Œ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144539705-dd0a62eb-9140-4ed3-8928-536bf6590804.png)

### fequently word

ë‹¨ì–´ ìˆ˜ë¥¼ ì œí•œí•˜ê¸° ìœ„í•´ì„œ ì¶œí˜„ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ì„ ìœ„ì£¼ë¡œ tokenzingì„ í•˜ëŠ” ê²ƒì€ ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144539738-7071ea88-ca0a-4eca-b644-138759e0a338.png)

í•˜ì§€ë§Œ, ì´ë ‡ê²Œ ë  ê²½ìš° OOV(out of vocabulary)ë¬¸ì œì—ì„œ ììœ ë¡­ì§€ ëª»í•©ë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/50165842/144540125-a26340ba-d0b1-429f-a58d-049264086b57.png)

ë‹¨ì–´ì¥ì— malapromismsë¼ëŠ” ë‹¨ì–´ê°€ ì—†ê¸°ì— ì •ë³´ì˜ ì†ì‹¤(loss of information) ì´ ë°œìƒí•©ë‹ˆë‹¤. Representation ëŠ¥ë ¥ì´ ë–¨ì–´ì§„ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.![image](https://user-images.githubusercontent.com/50165842/144540337-88941576-3459-4e24-8f8f-792999703c62.png)

UNKNOWN WORDSì— ëŒ€í•´ì„œ ë˜‘ê°™ì€ representationì„ ê°€ì§€ë¯€ë¡œ ì‚¬ëŒì˜ ê¸°ì¤€ì—ì„œ ë´¤ì„ ë•ŒëŠ” ë‹¤ë¥´ì§€ë§Œ, input_idsë¡œ ë³€í™˜ì„ í•œ ê²½ìš° ê°™ì€ í‘œí˜„ì„ ê°€ì§€ëŠ” ì—¬ëŸ¬ ë¬¸ì¥ì´ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë¬¸ì¥ì— ëŒ€í•´ì„œ ê°™ì€ representationì„ ê°€ì§€ë¯€ë¡œ ì •ë³´ì˜ ì†ì‹¤ì´ ë°œìƒí•œë‹¤ê³  ì´í•´í•˜ì‹œë©´ ì¢‹ì„ ë“¯ í•©ë‹ˆë‹¤.

### punctation representation

ìœ„ì˜ ì–˜ê¸°ì™€ ë¹„ìŠ·í•œ ë§¥ë½ì´ê¸´ í•˜ì§€ë§Œ ë‹¤ë¥¸ categoryë¼ ìƒê°í•˜ê³  ë¶„ë¥˜í•´ë´¤ìŠµë‹ˆë‹¤.

```python
"Don't you love ğŸ¤— Transformers? We sure do."

["Don't", "you", "love", "ğŸ¤—", "Transformers?", "We", "sure", "do."]
```

ë¬¸ì¥ê³¼ space-based splitì„ í•œ í† í°ë“¤ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ê²ƒì€  "Transformers?"ì™€ "do." ì…ë‹ˆë‹¤. ì´ ë‘ ê°œì˜ í† í° ë’¤ì—ëŠ” ë¬¸ì¥ë¶€í˜¸ê°€ ë¶™ì–´ìˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œì˜ dog, dogsì˜ ì˜ˆì‹œì²˜ëŸ¼ ë¬¸ì¥ë¶€í˜¸ ë‹¨ì–´ì˜ ì¡°í•©ì€ ë§ê¸° ë•Œë¬¸ì— vocabularyì˜ ì¡°í•©ì„ ì¦ê°€ì‹œí‚¤ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ”, large sizeì˜ modelë¡œ ì´ì–´ì§€ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ punctuation-based splitì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
["Don", "'", "t", "you", "love", "ğŸ¤—", "Transformers", "?", "We", "sure", "do", "."]
```

í•˜ì§€ë§Œ, ì´ ë°©ë²•ì—ë„ ë‹¨ì ì€ ìˆìŠµë‹ˆë‹¤. "Don't"ë¼ëŠ” í† í°ì€ "Don" ,"'","t"ë¡œ í† í¬ë‚˜ì´ì§•ì´ ë©ë‹ˆë‹¤. "Don't" ëŠ” "do not"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, "Do" , "n' t" ë¡œ í† í¬ë‚˜ì´ì§• ë˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œë¶€í„° caseê°€ ë³µì¡í•´ì§€ê¸° ì‹œì‘í•©ë‹ˆë‹¤. ê° ëª¨ë¸ë“¤ì´ ê³ ìœ í•œ tokenizerë¥¼ ê°€ì§€ê³  ìˆëŠ” ì´ìœ ì´ê¸°ë„ í•©ë‹ˆë‹¤. ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ ë‹¤ë¥¸ ê·œì¹™(tokenizer)ì„ ì ìš©í•˜ê²Œ ë˜ë©´ ë‹¤ë¥¸ representation(tokení™”ëœ text)ì´ ê²°ê´ê°’ìœ¼ë¡œ ë„ì¶œë©ë‹ˆë‹¤.PLM(Pretrained Language Model) ì€ training dataë¥¼ í† í°í™”í•˜ëŠ”ë° ì‚¬ìš©í•œ ê·œì¹™ì„(Tokenizer) ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•ŠìŠµë‹ˆë‹¤.  

### summary

ì´ëŸ¬í•œ rule-based tokenizerì˜ ìœ ëª…í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ”[spaCy](https://spacy.io/)ì™€ [Moses](http://www.statmt.org/moses/?n=Development.GetStarted)ê°€ ìˆìŠµë‹ˆë‹¤. ìœ ëª…í•œ ëª¨ë¸ë¡œëŠ”  [Transformer XL](https://huggingface.co/docs/transformers/master/en/model_doc/transformerxl) ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ vocabulary sizeëŠ” ë¬´ë ¤ 267,735ì…ë‹ˆë‹¤. ë³´í†µ, big size vocabularyëŠ” embedding layerì˜ sizeê°€ ì»¤ì§€ëŠ” ê²ƒì„ ì•¼ê¸°í•˜ê¸° ë•Œë¬¸ì— time complexity ì™€ memory ì‚¬ìš©ëŸ‰ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ë³´í†µ , ë‹¨ì¼ì–¸ì–´ëª¨ë¸ì€ 50000ê°œ ë³´ë‹¤ ì‘ì€ vocabualry size ë¥¼ ê°€ì§‘ë‹ˆë‹¤.  





## Character-level

ë¬¸ì ë‹¨ìœ„ë¡œ tokenizingì„ í•˜ëŠ” tokenizerì…ë‹ˆë‹¤.  ì¥ì ìœ¼ë¡œëŠ” vocabulary sizeê°€ ì‘ë‹¤ëŠ” ê²ƒì— ìˆìŠµë‹ˆë‹¤. 



### small size vocabulary

![image](https://user-images.githubusercontent.com/50165842/144560709-91809e82-6e5d-4663-917d-51b7f9e09e37.png)

out-of-vocabulary issueê°€ ìƒëŒ€ì ìœ¼ë¡œ ëœ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144560588-ef680f88-3c17-4fe2-8b9c-571c9343a28e.png)



ì•„ê¹Œ ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” , malapromismsë¥¼ UNKNOWN í† í°ìœ¼ë¡œ ë¶„ë¥˜í–ˆì§€ë§Œ, character-level tokenizerëŠ” ê·¸ ë‹¨ì–´ë¥¼ í† í°í™” í•©ë‹ˆë‹¤.

### poor representation

ì´ì— ë”°ë¥¸ ë‹¨ì ë„ ì¡´ì¬í•©ë‹ˆë‹¤. ìš°ì„ , ***<u>single characterì˜ meaningful representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤</u>***. ì˜ˆë¥¼ ë“¤ë©´ , 'a'ë¼ëŠ” characterì˜ representationì„ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ 'apple'ì´ë¼ëŠ” ë‹¨ì–´ì˜ representationì„ í•™ìŠµí•˜ê¸°ê°€ ë” ì‰½ìŠµë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ ì˜ˆë¡œëŠ”, ìœ„ ì‚¬ì§„ì—ì„œ 'l'ì´  ë§ì€ ì •ë³´ë¥¼ ë³´ìœ í•œë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒì€ ëª¨ë“  ìƒí™©ì— ëŒ€í•´ì„œ ì‚¬ì‹¤ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œì ê°™ì€ í‘œì˜ ë¬¸ìì˜ ê²½ìš° ë¬¸ì ìì²´ê°€ ë§ì€ ì˜ë¯¸ë¥¼ ë‹´ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ,  ë¡œë§ˆì, í•œê¸€, ì˜ì–´ ê°™ì€ í‘œìŒë¬¸ìëŠ” ê° ë¬¸ì ìì²´ê°€ ë§ì€ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.

### large size sequence

![image](https://user-images.githubusercontent.com/50165842/144560651-caa6d214-539c-432f-af95-47ab4bd65d0b.png)

í† í°ì˜ ìˆ«ìê°€ ë§ì´ ë°˜í™˜ë˜ê¸° ë•Œë¬¸ì—, sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§‘ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ transformer ê³„ì—´ë“¤ì€ ***<u>sequenceì˜  lengthê°€ ì œí•œ</u>***ì´ ë˜ì—ˆê¸°ì— truncation ì´ ë˜ê³  ***<u>token í™” ëœ ë¬¸ì¥ì€ ì›ë˜ ë¬¸ì¥ì˜ ì •ë³´ë¥¼ ë‹¤ ë‹´ì§€ ëª»í•˜ê²Œ ë©ë‹ˆë‹¤</u>***.



## Subword Tokenizer

![image](https://user-images.githubusercontent.com/50165842/145212853-38d249f5-0910-4201-aaf1-7ac7c9ef4ad4.png)

word-basedëŠ” ë§ì€ ë‹¨ì–´ì¥, semantic ì´ ë¹„ìŠ·í•˜ê³  , pucntationì´ ë‹¤ë¥¸  ë‹¨ì–´ë“¤ì„ ë§ì´ ë³µì œë¥¼ í•˜ê²Œ ë©ë‹ˆë‹¤.large sizeì˜ vocabì´ì—¬ì„œ ë¹ˆë„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ vocab sizeë¥¼ ì œí•œí•˜ê²Œ ë˜ë©´ out of vocabulary(OOV)ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.  ë˜í•œ ì˜ë¯¸ê°€ ê°™ì§€ë§Œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì´ vocabì•ˆì— ë“¤ì–´ê°€ì§€ ëª»í•´ì„œ , ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë“¤ì´ OOVë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. character levelì€ single characterê°€ meaningful representation ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³ ,  language ëª¨ë¸ì€ sequenceì˜ sizeê°€  ì œí•œì´ ë˜ì–´ìˆê¸° ë•Œë¬¸ì— , truncationì´ ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.subwordëŠ” ë‘˜ì˜ ì¥.ë‹¨ì ì„ ëª¨ë‘ ê³ ë ¤í•œ tokenizing ë°©ì‹ì…ë‹ˆë‹¤.

ì˜ë¯¸ê°€ ìˆëŠ” ë‹¨ì–´ë“¤(meaningful words)ì€ split ë˜ë©´ ì•ˆë˜ê³ , ë¹ˆë„ê°€ ë‚®ì€ ë‹¨ì–´ë“¤(rare words) ë“¤ì€ ì˜ë¯¸ ë‹¨ìœ„ë¡œ split ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.



![image](https://user-images.githubusercontent.com/50165842/145217029-0e0e132e-8686-4f79-824a-10a7008f75e9.png)

ì˜ˆë¥¼ ë“¤ì–´ , dogë¼ëŠ” ë‹¨ì–´ëŠ” ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ ì´ë¯€ë¡œ , split ë˜ë©´ ì•ˆë˜ê³  , dogsëŠ” dog , së¡œ split ë˜ì–´ì•¼ í•  ê²ƒì…ë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/50165842/145213747-c37d16cd-b62b-4464-94d7-624b73125a2c.png)

subwordë°©ì‹ì€ tokenizationì´ë¼ëŠ” ë‹¨ì–´ë¥¼ tokenizingì„ í•  ë•Œ , tokenê³¼ izationìœ¼ë¡œ í† í°í™”ê°€ ë  ê²ƒì…ë‹ˆë‹¤. 

ì¦‰ ,subwordë°©ì‹ì€ sytatic(ë¬¸ë²•) ê³¼ ê´€ë ¨ëœ í† í°ê³¼ , semantic(ì˜ë¯¸) ì™€ ê´€ë ¨ëœ í† í°ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ”ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/145215784-4d4c34c6-1eb0-436a-8408-7c824b66ce05.png)





Subword í† í¬ë‚˜ì´ì €ëŠ” ë‹¨ì–´ì˜ start í† í°ì¸ì§€ end í† í°ì¸ì§€ êµ¬ë³„ì„ í•´ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ , BertëŠ” ë‹¨ì–´ ì•ˆì˜ start tokenì¸ì§€ end tokenì¸ì§€ ## ì„ ì´ìš©í•´ì„œ êµ¬ë³„ì„ í•´ì¤ë‹ˆë‹¤. ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë“¤ì€ ë‹¤ë¥¸ ì ‘ë¯¸ì‚¬(prefix)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/145217887-2de741ba-cf74-4152-8ad3-37c8834cb225.png)

Bert ì™¸ì—ë„ ë‹¤ì–‘í•œ subword tokenizerë¥¼ ì‚¬ìš©í•˜ëŠ” PLMë“¤ì´ ìˆìŠµë‹ˆë‹¤. ***<u>Subword í† í¬ë‚˜ì´ì €ëŠ” vocabì˜ sizeë¥¼ ì¤„ì—¬ì£¼ê³  , prefixì™€ suffixë¥¼ ì‚¬ìš©í•´ì„œ ì–´ëŠ ë‹¨ì–´ì˜ í† í°ì¸ì§€ ëª…ì‹œë¥¼ í•´ì¤ë‹ˆë‹¤.</u>***



## BPE(Byte-pair-encoding)

BPEëŠ”  [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909) ì—ì„œ ì²˜ìŒìœ¼ë¡œ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.

GPT 2, RobertaëŠ” space-split í† í¬ë‚˜ ì´ìë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ì¢€ ë” ë³µì¡í•œ rule-based í† í¬ë‚˜ì´ì €(Moses)ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ë¡œëŠ” XLM, FlauBertê°€ ìˆìŠµë‹ˆë‹¤. GPTëŠ” Spaceë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. BPEëŠ” ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì„ ê±°ì¹˜ê²Œ ë©ë‹ˆë‹¤.

1. word-based, punct-based ê°™ì€ rule-based tokenizerë¡œ pre-tokenizingì„ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³ , ê° wordì˜ ë¹ˆë„ìˆ˜(frequency)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. 
2. ì´ ë‹¨ì–´ë“¤ì—ì„œ ë“±ì¥í•œ ëª¨ë“  symbol(character?)ë“¤ì„ í¬í•¨í•˜ëŠ” base vocabularyë¥¼ ë§Œë“­ë‹ˆë‹¤. 
3. two symbol(2-gram)ë“¤ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ vocabì— ì¶”ê°€í•©ë‹ˆë‹¤. (ì´ë¥¼ merge-ruleì´ë¼ê³  í•©ë‹ˆë‹¤)
4. ì›í•˜ëŠ” ë§Œí¼ ë‹¨ì–´ì˜ ìˆ˜(desired vocbualry size)ê°€ ìƒê¸°ë©´ mergeë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤. 

ì¦‰, desired vocabulary size(number of merge) ì™€ base-vocab sizeê°€ hyperparameterê°€ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. 

```python
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

pre-tokenization í›„ ì´ë ‡ê²Œ ë‹¨ì–´ê°€ ë¶„ë¦¬ê°€ ë˜ì—ˆë‹¤ê³  í•˜ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ, base vocabì€ "h","u" ,"p","g","n","s","b" ê°€ ë©ë‹ˆë‹¤.

```python
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ì—¬ê¸°ì„œ, two symbolì˜ frequencyê°€ ê°€ì¥ ë†’ì€ ê²ƒì€ "u" + "g" (15+5)ê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, tokenizerê°€ ì¶”ê°€í•˜ëŠ” ì²«ë²ˆì§¸ ë‹¨ì–´ëŠ”(merge rule) "ug" ê°€ ë©ë‹ˆë‹¤. 

```python
("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

ë‹¤ìŒ ë¹ˆë„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ two symoblì€ "u" + "n" ì´ ë©ë‹ˆë‹¤.(16ë²ˆ) ë”°ë¼ì„œ, "un" ì´ ë‹¨ì–´ì¥ì— ì¶”ê°€ê°€ ë©ë‹ˆë‹¤. ê²°êµ­ì—ëŠ” , ë‹¨ì–´ì¥ì€ `["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]` ê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì˜ˆë¥¼ë“¤ì–´ "bug"ì˜ ê²½ìš°  "b"  "ug"ë¡œ í† í¬ë‚˜ì´ì§•ì´ ë  ìˆ˜ ìˆì§€ë§Œ , mug" ì˜ ê²½ìš° "UNK" , "ug" ë¡œ í† í¬ë‚˜ì´ì§•ì´ ë  ê²ƒì…ë‹ˆë‹¤. GPTëŠ” base vocab ì„ 478 ê°œ , merge ruleì„ 4ë§Œê°œë¥¼ hyperparameterë¡œ ì„¤ì •í•˜ì—¬ ì´ 40478ê°œì˜ vocab ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.

### Byte-level Bpe

ê°€ëŠ¥í•œ ëª¨ë“  ê¸°ë³¸ ë¬¸ìë¥¼ base vocabì— ë„£ê²Œ ë˜ë©´ ìƒë‹¹í•œ í° ì‚¬ì´ì¦ˆê°€ ë ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´, ëª¨ë“  unicode ë¬¸ìê°€ base vocabì— í¬í•¨ë  ìˆ˜ ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, GPT2ëŠ” ë” ë‚˜ì€ base vocabì„ ê°€ì§€ê¸° ìœ„í•´ì„œ , bytesë¥¼ base vocabìœ¼ë¡œ ì‚¬ìš©í•˜ê³  , base vocabì˜ sizeê°€ 256ì´ ë˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ë§Œ, base characterë“¤ì€ base vocabì•ˆì— í¬í•¨ì´ ë˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤. GPT2ëŠ” ë¬¸í—Œìƒì˜ ì„¤ëª…ìœ¼ë¡œëŠ” ë¬¸ì¥ë¶€í˜¸ì— íŠ¹ë³„í•œ ruleì„ ì ìš©í•˜ì—¬ <unk>symbol ì—†ì´ ëª¨ë“  textë¥¼ tokenizeë¥¼ í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. GPT2ëŠ” 50257ì˜ vocab sizeë¥¼ ê°€ì§€ëŠ”ë°, ì´ëŠ” base vocab sizeë¡œ 256 , special end tokenìœ¼ë¡œ 1ê°œ , í•™ìŠµí•œ merge rules 50000ê°œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### wordpiece

### sentencepiece
