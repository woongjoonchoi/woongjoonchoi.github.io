---
title : "Huggingface tutorial: Tokenizer summary"



excerpt: "Tokenizer summary"

categories:
  - Huggingface
tags:
  - [Machine Learning,Huggingface,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---

## Huggingface tutorial ì‹œë¦¬ì¦ˆ : tokenizer

Huggingface tutorial ì‹œë¦¬ì¦ˆì¤‘ tokenizer í¸ì„ ë“£ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)

## What is tokenizer

tokenizerë€ sentenceë¥¼ sub-word í˜¹ì€ word ë‹¨ìœ„ë¡œ ìª¼ê° í›„ ì´ë¥¼ look-up-tableì„ í†µí•´ input idsë¡œ ë³€í™˜í•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. Huggingface tutorialì—ì„œëŠ” íŠ¹íˆ transfomers ê¸°ë°˜ì˜ ëª¨ë¸ë“¤ì— ì‚¬ìš©ë˜ëŠ” tokenizerë¥¼ ì‚´í´ë³´ê²Œ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144427223-af53b57a-f908-4b5c-b584-a425113a9cde.png)



## word-based tokenizer

word-level ë‹¨ìœ„ë¡œ tokenizingí•˜ëŠ” tokenizerëŠ” ì—¬ëŸ¬ê°œê°€ ìˆìŠµë‹ˆë‹¤.

spaceë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizing í•˜ëŠ” tokenizer ì…ë‹ˆë‹¤. spaceë¥¼ ê¸°ì¤€ìœ¼ë¡œ splití•˜ê²Œ ë˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144427638-cb31e5ee-6438-484c-bb21-027ea69988e5.png)

ë¬¸ì¥ë¶€í˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizing í•˜ëŠ” ruleì„ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144427823-345b4540-0a7b-410b-9146-5179cf3f0a96.png)



ì´ë¥¼ look-up-tableì—ì„œ input_ids ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/50165842/144427467-50990986-433e-4005-a1eb-70126e6d0e0e.png)

word-basedì˜ ë¬¸ì œì ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

very large vocabulary size



![image](https://user-images.githubusercontent.com/50165842/144538921-1a218243-073e-4f1c-bcc2-aa80e0f3195c.png)

dog, dogsë¼ëŠ” ë‹¨ì–´ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ë‘ ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ì§€ë‹™ë‹ˆë‹¤. í•˜ì§€ë§Œ, split based tokenizerë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê°ê° ë‹¤ë¥¸ idsë¥¼ ê°€ì§„ ë‹¨ì–´ë¡œ look-up tableì— mappingì´ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ì—„ì²­ë‚œ í¬ê¸°ì˜ vocabularyê°€ ìƒê¸°ê²Œ ë©ë‹ˆë‹¤. ì´ ë§ì€ ë‹¨ì–´ë¥¼ embedding í•˜ê¸° ìœ„í•´ì„œ ë” ë§ì€ parameters ë“¤ì´ í•„ìš”í•˜ê²Œ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144539705-dd0a62eb-9140-4ed3-8928-536bf6590804.png)

### fequently word

ë‹¨ì–´ ìˆ˜ë¥¼ ì œí•œí•˜ê¸° ìœ„í•´ì„œ ì¶œí˜„ ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ì„ ìœ„ì£¼ë¡œ tokenzingì„ í•˜ëŠ” ê²ƒì€ ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144539738-7071ea88-ca0a-4eca-b644-138759e0a338.png)

í•˜ì§€ë§Œ, ì´ë ‡ê²Œ ë  ê²½ìš° OOV(out of vocabulary)ë¬¸ì œì—ì„œ ììœ ë¡­ì§€ ëª»í•©ë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/50165842/144540125-a26340ba-d0b1-429f-a58d-049264086b57.png)

ë‹¨ì–´ì¥ì— malapromismsë¼ëŠ” ë‹¨ì–´ê°€ ì—†ê¸°ì— ì •ë³´ì˜ ì†ì‹¤(loss of information) ì´ ë°œìƒí•©ë‹ˆë‹¤. Representation ëŠ¥ë ¥ì´ ë–¨ì–´ì§„ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.![image](https://user-images.githubusercontent.com/50165842/144540337-88941576-3459-4e24-8f8f-792999703c62.png)

UNKNOWN WORDSì— ëŒ€í•´ì„œ ë˜‘ê°™ì€ representationì„ ê°€ì§€ë¯€ë¡œ ì‚¬ëŒì˜ ê¸°ì¤€ì—ì„œ ë´¤ì„ ë•ŒëŠ” ë‹¤ë¥´ì§€ë§Œ, input_idsë¡œ ë³€í™˜ì„ í•œ ê²½ìš° ê°™ì€ í‘œí˜„ì„ ê°€ì§€ëŠ” ì—¬ëŸ¬ ë¬¸ì¥ì´ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë¬¸ì¥ì— ëŒ€í•´ì„œ ê°™ì€ representationì„ ê°€ì§€ë¯€ë¡œ ì •ë³´ì˜ ì†ì‹¤ì´ ë°œìƒí•œë‹¤ê³  ì´í•´í•˜ì‹œë©´ ì¢‹ì„ ë“¯ í•©ë‹ˆë‹¤.

### punctation representation

ìœ„ì˜ ì–˜ê¸°ì™€ ë¹„ìŠ·í•œ ë§¥ë½ì´ê¸´ í•˜ì§€ë§Œ ë‹¤ë¥¸ categoryë¼ ìƒê°í•˜ê³  ë¶„ë¥˜í•´ë´¤ìŠµë‹ˆë‹¤.

```python
"Don't you love ğŸ¤— Transformers? We sure do."

["Don't", "you", "love", "ğŸ¤—", "Transformers?", "We", "sure", "do."]
```

ë¬¸ì¥ê³¼ space-based splitì„ í•œ í† í°ë“¤ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ê²ƒì€  "Transformers?"ì™€ "do." ì…ë‹ˆë‹¤. ì´ ë‘ ê°œì˜ í† í° ë’¤ì—ëŠ” ë¬¸ì¥ë¶€í˜¸ê°€ ë¶™ì–´ìˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œì˜ dog, dogsì˜ ì˜ˆì‹œì²˜ëŸ¼ ë¬¸ì¥ë¶€í˜¸ ë‹¨ì–´ì˜ ì¡°í•©ì€ ë§ê¸° ë•Œë¬¸ì— vocabularyì˜ ì¡°í•©ì„ ì¦ê°€ì‹œí‚¤ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ”, large sizeì˜ modelë¡œ ì´ì–´ì§€ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ punctuation-based splitì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
["Don", "'", "t", "you", "love", "ğŸ¤—", "Transformers", "?", "We", "sure", "do", "."]
```

í•˜ì§€ë§Œ, ì´ ë°©ë²•ì—ë„ ë‹¨ì ì€ ìˆìŠµë‹ˆë‹¤. "Don't"ë¼ëŠ” í† í°ì€ "Don" ,"'","t"ë¡œ í† í¬ë‚˜ì´ì§•ì´ ë©ë‹ˆë‹¤. "Don't" ëŠ” "do not"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, "Do" , "n' t" ë¡œ í† í¬ë‚˜ì´ì§• ë˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œë¶€í„° caseê°€ ë³µì¡í•´ì§€ê¸° ì‹œì‘í•©ë‹ˆë‹¤. ê° ëª¨ë¸ë“¤ì´ ê³ ìœ í•œ tokenizerë¥¼ ê°€ì§€ê³  ìˆëŠ” ì´ìœ ì´ê¸°ë„ í•©ë‹ˆë‹¤. ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ ë‹¤ë¥¸ ê·œì¹™(tokenizer)ì„ ì ìš©í•˜ê²Œ ë˜ë©´ ë‹¤ë¥¸ representation(tokení™”ëœ text)ì´ ê²°ê´ê°’ìœ¼ë¡œ ë„ì¶œë©ë‹ˆë‹¤.PLM(Pretrained Language Model) ì€ training dataë¥¼ í† í°í™”í•˜ëŠ”ë° ì‚¬ìš©í•œ ê·œì¹™ì„(Tokenizer) ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•ŠìŠµë‹ˆë‹¤.  

### summary

ì´ëŸ¬í•œ rule-based tokenizerì˜ ìœ ëª…í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ”[spaCy](https://spacy.io/)ì™€ [Moses](http://www.statmt.org/moses/?n=Development.GetStarted)ê°€ ìˆìŠµë‹ˆë‹¤. ìœ ëª…í•œ ëª¨ë¸ë¡œëŠ”  [Transformer XL](https://huggingface.co/docs/transformers/master/en/model_doc/transformerxl) ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ vocabulary sizeëŠ” ë¬´ë ¤ 267,735ì…ë‹ˆë‹¤. ë³´í†µ, big size vocabularyëŠ” embedding layerì˜ sizeê°€ ì»¤ì§€ëŠ” ê²ƒì„ ì•¼ê¸°í•˜ê¸° ë•Œë¬¸ì— time complexity ì™€ memory ì‚¬ìš©ëŸ‰ì„ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ë³´í†µ , ë‹¨ì¼ì–¸ì–´ëª¨ë¸ì€ 50000ê°œ ë³´ë‹¤ ì‘ì€ vocabualry size ë¥¼ ê°€ì§‘ë‹ˆë‹¤.  





## Character-level

ë¬¸ì ë‹¨ìœ„ë¡œ tokenizingì„ í•˜ëŠ” tokenizerì…ë‹ˆë‹¤.  ì¥ì ìœ¼ë¡œëŠ” vocabulary sizeê°€ ì‘ë‹¤ëŠ” ê²ƒì— ìˆìŠµë‹ˆë‹¤. 



### small size vocabulary

![image](https://user-images.githubusercontent.com/50165842/144560709-91809e82-6e5d-4663-917d-51b7f9e09e37.png)

out-of-vocabulary issueê°€ ìƒëŒ€ì ìœ¼ë¡œ ëœ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/50165842/144560588-ef680f88-3c17-4fe2-8b9c-571c9343a28e.png)



ì•„ê¹Œ ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” , malapromismsë¥¼ UNKNOWN í† í°ìœ¼ë¡œ ë¶„ë¥˜í–ˆì§€ë§Œ, character-level tokenizerëŠ” ê·¸ ë‹¨ì–´ë¥¼ í† í°í™” í•©ë‹ˆë‹¤.

### poor representation

ì´ì— ë”°ë¥¸ ë‹¨ì ë„ ì¡´ì¬í•©ë‹ˆë‹¤. ìš°ì„ , single characterì˜ meaningful representationì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ , 'a'ë¼ëŠ” characterì˜ representationì„ í•™ìŠµí•˜ëŠ” ê²ƒë³´ë‹¤ 'apple'ì´ë¼ëŠ” ë‹¨ì–´ì˜ representationì„ í•™ìŠµí•˜ê¸°ê°€ ë” ì‰½ìŠµë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ ì˜ˆë¡œëŠ”, ìœ„ ì‚¬ì§„ì—ì„œ 'l'ì´  ë§ì€ ì •ë³´ë¥¼ ë³´ìœ í•œë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒì€ ëª¨ë“  ìƒí™©ì— ëŒ€í•´ì„œ ì‚¬ì‹¤ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œì ê°™ì€ í‘œì˜ ë¬¸ìì˜ ê²½ìš° ë¬¸ì ìì²´ê°€ ë§ì€ ì˜ë¯¸ë¥¼ ë‹´ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ,  ë¡œë§ˆì, í•œê¸€, ì˜ì–´ ê°™ì€ í‘œìŒë¬¸ìëŠ” ê° ë¬¸ì ìì²´ê°€ ë§ì€ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.

### large size sequence

![image](https://user-images.githubusercontent.com/50165842/144560651-caa6d214-539c-432f-af95-47ab4bd65d0b.png)

í† í°ì˜ ìˆ«ìê°€ ë§ì´ ë°˜í™˜ë˜ê¸° ë•Œë¬¸ì— , sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§‘ë‹ˆë‹¤.ëŒ€ë¶€ë¶„ transformer ê³„ì—´ë“¤ì€ sequenceì˜  lengthê°€ ì œí•œì´ ë˜ì—ˆê¸°ì— truncation ì´ë˜ê³  tokení™” ëœ ë¬¸ì¥ì€ ì›ë˜ ë¬¸ì¥ì˜ ì •ë³´ë¥¼ ë‹¤ ë‹´ì§€ ëª»í•˜ê²Œ ë©ë‹ˆë‹¤.



