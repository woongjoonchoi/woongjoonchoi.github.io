---
title : "Efficient Attention"



excerpt: "Fused Kernel"

categories:
  - Kernel_Fusion
tags:
  - [GPU Programming,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---
## Introduction

Transformer 구조의 핵심인 **Self-Attention**은 원래 *Attention is All You Need* 논문에서 처음 등장한 이후, 다양한 자연어 처리(NLP)와 컴퓨터 비전(CV) 모델에서 널리 사용되고 있습니다.  
특히, Self-Attention은 시퀀스 전체의 임베딩을 **병렬적으로 계산할 수 있다는 장점** 덕분에, 더 큰 컨텍스트를 한 번에 처리할 수 있는 **대규모 언어 모델(LLM, Large Language Model)**의 기반이 되었습니다.

하지만 LLM의 규모가 커질수록 Self-Attention이 모델 전체 실행 비용에서 차지하는 비중도 커지게 됩니다. 다시 말해, **Self-Attention 연산을 얼마나 효율적으로 구현하느냐에 따라 모델의 속도, 메모리 사용량, 비용이 크게 달라질 수 있다는 뜻입니다.**

이에 따라 최근에는 기존의 Self-Attention과 **수학적으로 동일한 결과를 내지만, 훨씬 더 메모리 효율적인(Memory Efficient) 방식으로 구현하는 방법**이 주목받고 있습니다.  
이러한 접근법은 연산량을 줄이거나, 중간 결과를 저장하는 메모리 사용을 최소화하여, 대규모 모델을 보다 빠르고 효율적으로 실행할 수 있도록 합니다.

Self-Attention의 효율적인 구현은 단순한 최적화를 넘어, LLM의 실용성을 좌우하는 핵심 기술 중 하나가 되었습니다. 앞으로도 다양한 방식의 Memory Efficient Attention 기법들이 등장하며, 더욱 빠르고 강력한 모델들이 개발될 것으로 기대됩니다.



## Background

### Attention

기존의 RNN, GRU, LSTM 기반의 encoder-decoder 구조는 시퀀스 정보를 고정된 크기의 벡터에 압축하면서 **정보 손실**이 발생하는 한계가 있었습니다. 이를 보완하기 위해 **Attention** 메커니즘이 등장했습니다.  
Attention은 [Bahdanau et al. (2014)](https://arxiv.org/pdf/1409.0473)이 처음 제안했으며, 해당 연구는 뉴욕대학교의 조경현 교수님이 주도했습니다.

조 교수님은 [강의 자료](https://www.boostcourse.org/ai331/lecture/540234?isDesc=false)를 통해 "고정된 크기의 벡터로는 시퀀스의 의미를 완전히 표현할 수 없다"고 설명합니다.  
즉, 입력 시퀀스를 단일 벡터로 압축하는 것이 아니라, **시퀀스의 길이에 비례한 벡터 정보**를 decoder의 입력으로 활용해야 한다는 것입니다.

<br/>

![rnn-attention](\assets\images\DeepLearning\KernelFusion\rnn-attention.png)

입력 시퀀스는 토크나이징되어 임베딩 벡터의 시퀀스로 변환됩니다. 이 벡터들은 각 토큰의 의미를 담고 있으며, 다음 그림처럼 각 벡터의 차이와 방향성으로 관계를 나타낼 수 있습니다.

![vector-diff](\assets\images\DeepLearning\KernelFusion\vectordiff.png)  
![country-capital](\assets\images\DeepLearning\KernelFusion\country-capital.png)

이러한 벡터 표현의 유용성은 [Tomas Mikolov의 연구](https://arxiv.org/pdf/1310.4546), [ACL 논문](https://aclanthology.org/N13-1090.pdf) 등에서 입증되었습니다.  
벡터의 **방향과 크기**를 통해 단어 간 관계를 파악할 수 있으며, 임베딩 벡터들을 더하거나 평균내는 방식으로 **의미 있는 벡터**를 생성할 수도 있습니다.

즉, tokenize된 시퀀스는 임베딩 벡터들의 시퀀스로 표현되며, 이 벡터들을 조합하면 문맥 정보를 유지한 의미 벡터를 만들 수 있습니다.

하지만 중요한 점은, decoder가 생성하는 각 시점의 출력은 입력 임베딩 벡터 전체를 **동등하게** 활용하지 않는다는 것입니다.  
각 출력마다 어떤 입력 벡터가 더 중요한지를 판단하기 위해, decoder는 입력 벡터들과의 **유사도**를 계산하고, 이를 기반으로 가중합(weighted sum)을 수행합니다.  
이때 유사도는 보통 dot-product 기반으로 계산되며, 이를 확률 분포로 변환하기 위해 **softmax 함수**를 적용합니다.

<br/>

![attention-result](\assets\images\DeepLearning\KernelFusion\attention-result.png)

그 결과, decoder는 입력 시퀀스의 의미 정보를 **손실 없이 동적으로 선택**하여 활용할 수 있게 됩니다. 이는 고정된 컨텍스트 벡터의 한계를 극복하며, 자연어 처리 성능을 크게 향상시켰습니다.


#### Self-Attention

Self-Attention은 기존의 Attention 구조에서 **RNN을 Fully-Connected Network로 대체**한 방식입니다.  
이러한 구조 변경을 통해 RNN의 순차적인 계산을 피하고, **행렬 곱 연산(GEMM, General Matrix Multiplication)**을 사용할 수 있게 되었습니다.

GEMM은 병렬 처리가 가능하며, 하드웨어 수준에서 다양한 최적화를 적용할 수 있기 때문에 **RNN 기반 Attention보다 훨씬 빠른 연산이 가능**합니다.  
이로 인해 Self-Attention은 빠르게 주류로 자리잡았고, 이후 등장한 **GPT-2, GPT-3**와 같은 대규모 언어 모델(LLM)의 핵심 구조로 채택되었습니다.

또한, Self-Attention은 자연어 처리에 국한되지 않고 **Vision Transformer (ViT)** 와 같은 **비전 모달리티(vision modality)** 에도 성공적으로 적용되며, 시각 정보를 처리하는 방식에도 큰 변화를 일으켰습니다.

최근에는 Language, Vision 등 다양한 모달리티를 통합하는 **Large Multi-Modal Model**에서, Self-Attention이 그 중심 모듈로 사용되고 있습니다.  
즉, Self-Attention은 다양한 데이터를 이해하고 연결하는 데 있어 **범용적이고 확장 가능한 연산 구조**로 진화해가고 있습니다.


### Efficient Attention

Efficient Attention의 일반적인 접근 방법은 computation Complexity의 Upper Bound를 낮추는 방법입니다. 
context를 전체 input이 아닌 부분으로 한정하는 Window Attention 이 있습니다.Linear Attention은 Q@K^T를 계산할 때 (n,d) ,(d,n) 의 matrix들을 곱해서 계산하는게 아닌 K^T @V를 먼저 계산함. 즉, (d,n) @ (n,d) 의 matrix multiplication을 통해서 computation complexity를 O(N)으로 낮춥니다.
하지만, 이러한 efficient attention은 computation Complexity의 장점을 얻는 대신에 정확도에서 단점을 얻게 됩니다.
따라서, Memory Efficient한 Attention이 주목을 받고 있습니다. Gpu Programming을 활용하여 정확도를 유지한채 연산속도와 메모리 사용량을 상당부분 향상시키기 때문입니다.

## Softmax
Attention은 embedding vector들을 합하지만 모든 embedding vector를 동일하게 사용하는 것이 아니기에 가중치를 계산하게 됩니다. 이 때, 사용하는 것이 softmax입니다.
softmax는 받은 vector의 element의 합이 1이 되도록 , 각 element가 0이 되도록 변환을 해줍니다.
### Safe-Softmax

$$ y_i = \frac{e^{x_i}}{\sum_{j=1}^V e^{x_j}} $$
기존의 softmax는 exponential 함수를 사용하기에 overflow나 underflow 문제가 발생할 수 있다.

따라서,max값을 빼주어서 overflow나 underflow를 방지한다.
$$ y_i = \frac{e^{x_i - \max_{k=1}^V x_k}}{\sum_{j=1}^V e^{x_j - \max_{k=1}^V x_k}} $$


![online-softmax](\assets\images\DeepLearning\KernelFusion\safesoftmax.png)



### Online-Softmax 
하지만,softmax는 전체 row를 봐야한다.
전체 row를 메모리에 매번 적재를 하는 것은 부담이 됩니다.
따라서, input의 일부만을 확인하고 매번 분모와 분자를 update한다.

![online-softmax](\assets\images\DeepLearning\KernelFusion\online-softmax.png)


#### Proof 

**Base case**: \(V = 1\)

1. $$m_1 \leftarrow x_1$$  
   $$= \max_{k=1}^1 x_k$$

2. $$d_1 \leftarrow e^{x_1 - m_1}$$  
   $$= \sum_{j=1}^1 e^{x_j - m_1}$$


**Induction**: \(V = S\)

1. $$ Assume \quad m_{S-1} = \max_{k=1}^{S-1} x_k $$  
$$m_S \leftarrow \max(m_{S-1}, x_S) $$  
$$= \max\bigl(\max_{k=1}^{S-1} x_k,\; x_S\bigr) $$  
$$ = \max_{k=1}^{S} x_k $$  



2. $$ Assume \quad d_{S-1} = \sum_{j=1}^{S-1} e^{\,x_j - m_S} $$  
$$d_S \leftarrow d_{S-1} \, e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$= \left(\sum_{j=1}^{S-1} e^{\,x_j - m_{S-1}}\right) e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$ = \sum_{j=1}^{S} e^{\,x_j - m_S} $$

#### SafeNess 

- $$m_j \in \Bigl[\min_{k=1}^V m_k,\; \max_{k=1}^V m_k\Bigr], \quad \forall j \in \{1, \dots, V\}$$  
  따라서,, $$m_j$$ 는 overflow나 underflow가 될 수 없습니다.

- $$1 \;\le\; d_j \;\le\; j, \quad \forall j \in \{1, \dots, V\}$$  
  $$d_j $$의 식은 다음과 같습니다.  
  $$d_S \leftarrow d_{j-1} \, e^{\,m_{j-1} - m_j} + e^{\,x_j - m_j} $$  
  매번 1보다 같거나 작은 수를 $$d_j $$에 곱하고 $$d_j $$에 더해줍니다.  
  따라서, 32-bit floating point 는 $$d_j$$가 $$1.7 \times 10^{37}$$ 까지 저장할 수 있도록 보장해줍니다.
  왜냐하면, 32-bit floating point 는 $$ 3.4 \times 10^{38}$$ 까지 표현할 수 있기 때문입니다. 
  32-bit floating point의 range보다 적은 값까지 표현이 가능하다 하는 이유는 안전수치를 보수적으로 잡은거 같습니다. 
  만약에, 더 많은 벡터를 처리한다면 64-bit floating point 를 사용해야 할 것입니다.  
  
#### Block-Update Proof
위에서 증명한 것들은 Online-Softmax를 1개의 data를 차례대로 계산하여 update하는 과정을 증명한 것입니다.  
실제로, data를 여러개 받아서 $$d_j $$ , $$m_j $$ 등을 계산하기 때문에 이를 기존에 구한 $$d_i $$ , $$m_i $$ 에 반영할 수 있어야 합니다.  

$$
\begin{bmatrix}
m_i \\
d_i
\end{bmatrix}
\;\oplus\;
\begin{bmatrix}
m_j \\
d_j
\end{bmatrix}
=
\begin{bmatrix}
\max(m_i, m_j) \\
d_i \, e^{\,m_i - \max(m_i, m_j)} \;+\; d_j \, e^{\,m_j - \max(m_i, m_j)}
\end{bmatrix}
$$

$$d_i $$ , $$m_i $$ 를 미리 구해놓고 다음 input에 대하여 $$d_j $$ , $$m_j $$ 를 계산하면 위의 공식에 대입하여 update를 할 수 있습니다.  
max에 대한 증명은 생략하도록 하겠습니다. 

1. $$ Assume \quad d_{j} = \sum_{k=0}^{j} e^{\,x_k - m_i} \quad d_{i} = \sum_{k=i+1}^{j+i} e^{\,x_k - m_j} \quad m_{i+j} = \max(m_i, m_j) $$  
$$d_i \oplus d_{j} = d_i \, e^{\,m_i - \max(m_i, m_j)} \;+\; d_j \, e^{\,m_j - \max(m_i, m_j)} $$  
$$= \left(\sum_{k=0}^{i} e^{\,x_k - m_{i}}\right) e^{\,m_{i} - \max(m_i, m_j)} + \left(\sum_{k=i+1}^{i+j} e^{\,x_k - m_{i}}\right) e^{\,m_{j} - \max(m_i, m_j)} $$  
$$= \left(\sum_{k=0}^{i} e^{\,x_k - m_{i}}\right) e^{\,m_{i} - m_{i+j}} + \left(\sum_{k=i+1}^{i+j} e^{\,x_k - m_{i}}\right) e^{\,m_{j} - m_{i+j}} $$  
$$ = \sum_{k=0}^{i+j} e^{\,x_k - m_{i+j}} $$





## Online-Self Attention
attention에는 softmax 때문에 input 전체를 봐야한다.

하지만, softmax는 Online-softmax로 전체를 안봐도 계산이 가능하다

self-attention에도 이를 적용한다.[[link]](https://arxiv.org/pdf/2112.05682) 
$$ v^* \in \mathbb{R}^d $$ ,$$ s^* \in \mathbb{R} $$ 를 0으로 초기화하고 , $$m^*$$를 -inf로 초기화합니다.
query $$q$$, keys $$k_1, \dots, k_n$$ 와 values $$v_1, \dots, v_n$$ 가 주어질 때 , keys와 values들을 순서대로 사용합니다.$$k_i$$ , $$v_i$$ 가 주어지면 $$ s_i = \mathrm{dot}(q, k_i) $$ 를 계산합니다.그리고 나서,$$m^* = \max(m^*,s_i)$$, $$ v^* \leftarrow v^* e^{m^* - m_i} + v_i e^{s_i - m_i} $$ , $$ s^* \leftarrow s^* e^{m^* - m_i} + e^{s_i - m_i} $$ 를  update합니다.그리고,$$ \frac{v^*}{s^*} $$를 계산해줍니다.



### Proof 

$$m^* = \max(m^*,s_i)$$, ,$$ s^* \leftarrow s^* e^{m^* - m_i} + e^{s_i - m_i} $$ 들은 위에서 증명이 되었습니다.  
따라서, 증명을 생략하도록 하겠습니다. $$ v^* \leftarrow v^* e^{m^* - m_i} + v_i e^{s_i - m_i} $$에 대해서 증명을 하도록 하겠습니다.  

**Base case**: \(V = 1\)  

1. $$v^* \leftarrow v^* e^{m^* - m_1} + v_1 e^{s_1 - m_1} $$  
   $$= v_1 e^{s_1 - m_1}$$


**Induction**: \(V = i\)  



1. $$ Assume \quad v^* = \sum_{j=1}^{i-1} v_je^{\,x_j - m^*} $$  
$$ m_i = \max(m^*,s_i)$$ , $$ s_i = \mathrm{dot}(q, k_i) $$  
$$ v^* \leftarrow v^* e^{m^* - m_i} + v_i e^{s_i - m_i} $$  
$$= \left(\sum_{j=1}^{i-1} v_j e^{\,x_j - m^*}\right) e^{\,m^* - m_i} + v_i e^{\,s_i - m_i} $$  
$$ = \sum_{j=1}^{i} v_j  e^{\,s_j - m_i} $$  
그리고, $$ \frac{v^*}{s^*} $$ 를 계산해줍니다.  
$$m_i = \max(m^*,s_i)$$ 가 현재까지 계산한 $$ \max $$ 이기 때문에 online self-attention 수식이 정확함이 증명이 됩니다.


## FlashAttention



Online-Self Attention을 실제로 구현체를 Open-source로 제공한 것이 FlashAttention이다.
FlashAttention은 Tri Dao라는 사람이 제안을 하였습니다.FlashAttention의 알고리즘에서 idea가 새로 나온것은 없지만 Online-Self Attention을 실제로 여러 LLM에 적용할 수 있도록 구현체를 제공한 최초의 사례입니다. 

### Forward
![FlashAttention](\assets\images\DeepLearning\KernelFusion\FlashAttention_forward.png)  

FlashAttention의 forward는 이와 같습니다. algorithm의 line별로 설명을 하도록 하겠습니다.

1.  $$B_r$$는 $$Query$$의 block size를 의미하고 , $$B_c$$ 는 $$Key,Value$$의 block size를 의미합니다.

2. 위의 online-self attention 에서는 $$ v^* \in \mathbb{R}^d $$ ,$$ s^* \in \mathbb{R} $$ 를 0으로 초기화 했는데, FlashAttention에서는 이를 $$ O , \ell $$ 로 각각 표기법을 바꾸었습니다. Batch dimension으로 확장했기에 $$ O = \mathbf{0}_{N\times d} \in \mathbb{R}^{N \times d} $$ ,$$ \ell = \mathbf{0}_N \in \mathbb{R}^N $$ ,$$m = (-\infty)_N \in \mathbb{R}^N $$ 로 초기화합니다.

3. $$Query ,key , value$$ 를 여러 block으로 쪼갭니다.

4. $$O$$는 $$Query$$ 의 weighted sum이므로 $$Query$$ 와 같은 Block size인 $$B_r$$ 으로 쪼개고, $$ \ell , m $$는 softmax를 계산할 때 사용되는 중간계산결과 이므로 $$Key,Value$$와 같은 Block size인 $$B_c$$ 를 사용합니다.

5. 모든 $$Key,Value$$ Block에 대해서 반복합니다.

6. $Key,Value$의 block을 먼저 Cache에 load합니다. 

7. 모든 $$Query $$ Block에 대해서 반복합니다.

8. $$ Query,O, \ell , m $$의 block을 Cache에 load합니다.

### Backward


## Conclusion


$$ \mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^{T} \in \mathbb{R}^{B_r \times B_c} $$
## References