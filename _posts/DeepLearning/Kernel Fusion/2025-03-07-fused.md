---
title : "Efficient Attention"



excerpt: "Fused Kernel"

categories:
  - Kernel_Fusion
tags:
  - [GPU Programming,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---
## Introduction

Self-Attention은 Transformer 논문에서 처음 등장하여 현재까지 널리 쓰이고 있습니다.
Self-Attention은 병렬적으로 sequence의 embedding을 계산할 수 있는 점 때문에 한번에 계산하는 context를 키운 LLM으로 발전하게 됩니다.
LLM으로 발전하면서 모델의 실행 비용이 self-attention에 의존적으로 변합니다.따라서, self-attention을 효율적으로 구현하면 할 수록 모델의 실행 비용이 줄어들게 됩니다.
self-attention을 수학적으로 동일하지만 Memory Efficient하게 구현하는 방법이 주목을 받고 있습니다.


## BackGround

### Self-Attention


### Efficient Attention

Efficient Attention의 일반적인 접근 방법은 computation Complexity의 Upper Bound를 낮추는 방법입니다. 
context를 전체 input이 아닌 부분으로 한정하는 Window Attention 이 있습니다.Linear Attention은 Q@K^T를 계산할 때 (n,d) ,(d,n) 의 matrix들을 곱해서 계산하는게 아닌 K^T @V를 먼저 계산함. 즉, (d,n) @ (n,d) 의 matrix multiplication을 통해서 computation complexity를 O(N)으로 낮춥니다.
하지만, 이러한 efficient attention은 computation Complexity의 장점을 얻는 대신에 정확도에서 단점을 얻게 됩니다.
따라서, Memory Efficient한 Attention이 주목을 받고 있습니다. Gpu Programming을 활용하여 정확도를 유지한채 연산속도와 메모리 사용량을 상당부분 향상시키기 때문입니다.

## Online-Softmax

$$ y_i = \frac{e^{x_i}}{\sum_{j=1}^V e^{x_j}} $$
기존의 softmax는 exponential 함수를 사용하기에 overflow나 underflow 문제가 발생할 수 있다.

따라서,max값을 빼주어서 overflow나 underflow를 방지한다.
$$ y_i = \frac{e^{x_i - \max_{k=1}^V x_k}}{\sum_{j=1}^V e^{x_j - \max_{k=1}^V x_k}} $$


![online-softmax](\assets\images\DeepLearning\KernelFusion\safesoftmax.png)



하지만,softmax는 전체 row를 봐야한다.
전체 row를 메모리에 매번 적재를 하는 것은 부담이 됩니다.
따라서, input의 일부만을 확인하고 매번 분모와 분자를 update한다.

![online-softmax](\assets\images\DeepLearning\KernelFusion\online-softmax.png)

**Base case**: \(V = 1\)

1. $$m_1 \leftarrow x_1$$  
   $$= \max_{k=1}^1 x_k$$

2. $$d_1 \leftarrow e^{x_1 - m_1}$$  
   $$= \sum_{j=1}^1 e^{x_j - m_1}$$


## Online-Self Attention
attention에는 softmax 때문에 input 전체를 봐야한다.

하지만, softmax는 Online-softmax로 전체를 안봐도 계산이 가능하다

self-attention에도 이를 적용한다.

Online-Self Attention을 실제로 알고리즘 관점에서 자세하게 분석한게 FlashAttention이다.


귀납법으로 증명



## Conclusion

