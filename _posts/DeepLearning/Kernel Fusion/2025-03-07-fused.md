---
title : "Efficient Attention"



excerpt: "Fused Kernel"

categories:
  - Kernel_Fusion
tags:
  - [GPU Programming,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---
## Introduction

Self-Attention은 Transformer 논문에서 처음 등장하여 현재까지 널리 쓰이고 있습니다.
Self-Attention은 병렬적으로 sequence의 embedding을 계산할 수 있는 점 때문에 한번에 계산하는 context를 키운 LLM으로 발전하게 됩니다.
LLM으로 발전하면서 모델의 실행 비용이 self-attention에 의존적으로 변합니다.따라서, self-attention을 효율적으로 구현하면 할 수록 모델의 실행 비용이 줄어들게 됩니다.
self-attention을 수학적으로 동일하지만 Memory Efficient하게 구현하는 방법이 주목을 받고 있습니다.


## BackGround

### Attention
Attention은 기존의 RNN,GRU,LSTM 스타일의 sequence encoding의 한계점을 보완하기 위해 나왔습니다. 
Attention은 뉴옥대의 조경현 교수님께서 처음 제안하셨습니다. 조경현 교수님께서는 fixed size vector에 sequence vector를 압축하는것이 불가능하다고 결론을 내리셨습니다.
따라서, sequence vector를 fixed size로 압축해서  decoder의 input에 주지 않고 token length에  비례한 vector를 decoder에 input으로 주었습니다.
input sequence를 tokenize한 embedding vector들로 정보를 만들어야 합니다.
![vector-diff](\assets\images\DeepLearning\KernelFusion\vectordiff.png)  
![country-capital](\assets\images\DeepLearning\KernelFusion\country-captial.png)
embedding 된 sequence간의 relation은 vector의 성질중 하나인 방향과 크기가 같다면 같다라는 성질이 만족함이 보여졌습니다.
이는 embedding 들을 더하면 위치에 관계없이 어떤 의미를 나타내는 vector를 만들 수 있음을 의미합니다.
따라서, tokenize된 sequence는  embedding vector의 sequence라 볼 수 있고 이 embedding vector들을 더하면 어떠한 의미를 나타내게 됩니다. 
이 때, 어떤 output vector를 만들어 낼때 input embedding vector가 항상 동등하게 사용되는 것은 아닙니다.
따라서, decoder의 time step마다 어떤 embedding이 필요한지 계산하여 이 embedding vector와 input embedding vector의 유사도를 계산하여 필요한 vector를 weighted sum을 계산해줄려합니다.이때 , 유사도의 범위를 조정해주기 위해서 softmax function을 사용합니다.
이 때, softmax function을 통해서 
그 결과 , input sequence의 정보가 손실되지 않았습니다. 



#### Self Attention
Self-Attention은 기존의 Attention

### Efficient Attention

Efficient Attention의 일반적인 접근 방법은 computation Complexity의 Upper Bound를 낮추는 방법입니다. 
context를 전체 input이 아닌 부분으로 한정하는 Window Attention 이 있습니다.Linear Attention은 Q@K^T를 계산할 때 (n,d) ,(d,n) 의 matrix들을 곱해서 계산하는게 아닌 K^T @V를 먼저 계산함. 즉, (d,n) @ (n,d) 의 matrix multiplication을 통해서 computation complexity를 O(N)으로 낮춥니다.
하지만, 이러한 efficient attention은 computation Complexity의 장점을 얻는 대신에 정확도에서 단점을 얻게 됩니다.
따라서, Memory Efficient한 Attention이 주목을 받고 있습니다. Gpu Programming을 활용하여 정확도를 유지한채 연산속도와 메모리 사용량을 상당부분 향상시키기 때문입니다.

## Softmax

### Safe-Softmax

$$ y_i = \frac{e^{x_i}}{\sum_{j=1}^V e^{x_j}} $$
기존의 softmax는 exponential 함수를 사용하기에 overflow나 underflow 문제가 발생할 수 있다.

따라서,max값을 빼주어서 overflow나 underflow를 방지한다.
$$ y_i = \frac{e^{x_i - \max_{k=1}^V x_k}}{\sum_{j=1}^V e^{x_j - \max_{k=1}^V x_k}} $$


![online-softmax](\assets\images\DeepLearning\KernelFusion\safesoftmax.png)



### Online-Softmax 
하지만,softmax는 전체 row를 봐야한다.
전체 row를 메모리에 매번 적재를 하는 것은 부담이 됩니다.
따라서, input의 일부만을 확인하고 매번 분모와 분자를 update한다.

![online-softmax](\assets\images\DeepLearning\KernelFusion\online-softmax.png)


#### Proof 

**Base case**: \(V = 1\)

1. $$m_1 \leftarrow x_1$$  
   $$= \max_{k=1}^1 x_k$$

2. $$d_1 \leftarrow e^{x_1 - m_1}$$  
   $$= \sum_{j=1}^1 e^{x_j - m_1}$$


**Induction**: \(V = S\)

1. $$ Assume \quad m_{S-1} = \max_{k=1}^{S-1} x_k $$  
$$m_S \leftarrow \max(m_{S-1}, x_S) $$  
$$= \max\bigl(\max_{k=1}^{S-1} x_k,\; x_S\bigr) $$  
$$ = \max_{k=1}^{S} x_k $$  



2. $$ Assume \quad d_{S-1} = \sum_{j=1}^{S-1} e^{\,x_j - m_S} $$  
$$d_S \leftarrow d_{S-1} \, e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$= \left(\sum_{j=1}^{S-1} e^{\,x_j - m_{S-1}}\right) e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$ = \sum_{j=1}^{S} e^{\,x_j - m_S} $$

#### SafeNess 

**SafeNess** :  
- $$m_j \in \Bigl[\min_{k=1}^V m_k,\; \max_{k=1}^V m_k\Bigr], \quad \forall j \in \{1, \dots, V\}$$  
  따라서,, $$m_j$$ 는 overflow나 underflow가 될 수 없습니다.

- $$1 \;\le\; d_j \;\le\; j, \quad \forall j \in \{1, \dots, V\}$$  
  $$d_j $$의 식은 다음과 같습니다.  
  $$d_S \leftarrow d_{j-1} \, e^{\,m_{j-1} - m_j} + e^{\,x_j - m_j} $$  
  매번 1보다 같거나 작은 수를 $$d_j $$에 곱하고 $$d_j $$에 더해줍니다.  
  따라서, 32-bit floating point 는 $$d_j$$가 $$1.7 \times 10^{37}$$ 까지 저장할 수 있도록 보장해줍니다.
  왜냐하면, 32-bit floating point 는 $$ 3.4 \times 10^{38}$$ 까지 표현할 수 있기 때문입니다. 
  32-bit floating point의 range보다 적은 값까지 표현이 가능하다 하는 이유는 안전수치를 보수적으로 잡은거 같습니다. 
  만약에, 더 많은 벡터를 처리한다면 64-bit floating point 를 사용해야 할 것입니다.  
  
#### Block-Update Proof
위에서 증명한 것들은 Online-Softmax를 1개의 data를 차례대로 계산하여 update하는 과정을 증명한 것입니다.  
실제로, data를 여러개 받아서 $$d_j $$ , $$m_j $$ 등을 계산하기 때문에 이를 기존에 구한 $$d_i $$ , $$m_i $$ 에 반영할 수 있어야 합니다.  

$$
\begin{bmatrix}
m_i \\
d_i
\end{bmatrix}
\;\oplus\;
\begin{bmatrix}
m_j \\
d_j
\end{bmatrix}
=
\begin{bmatrix}
\max(m_i, m_j) \\
d_i \, e^{\,m_i - \max(m_i, m_j)} \;+\; d_j \, e^{\,m_j - \max(m_i, m_j)}
\end{bmatrix}
$$

$$d_i $$ , $$m_i $$ 를 미리 구해놓고 다음 input에 대하여 $$d_j $$ , $$m_j $$ 를 계산하면 위의 공식에 대입하여 update를 할 수 있습니다.
max에 대한 증명은 생략하도록 하겠습니다. 

1. $$ Assume \quad d_{j} = \sum_{k=0}^{j} e^{\,x_k - m_i} \quad d_{i} = \sum_{k=i+1}^{j+i} e^{\,x_k - m_j} \quad m_{i+j} = \max(m_i, m_j) $$  
$$d_i \oplus d_{j} = d_i \, e^{\,m_i - \max(m_i, m_j)} \;+\; d_j \, e^{\,m_j - \max(m_i, m_j)} $$  
$$= \left(\sum_{k=0}^{i} e^{\,x_k - m_{i}}\right) e^{\,m_{i} - \max(m_i, m_j)} + \left(\sum_{k=i+1}^{i+j} e^{\,x_k - m_{i}}\right) e^{\,m_{j} - \max(m_i, m_j)} $$  
$$= \left(\sum_{k=0}^{i} e^{\,x_k - m_{i}}\right) e^{\,m_{i} - m_{i+j}} + \left(\sum_{k=i+1}^{i+j} e^{\,x_k - m_{i}}\right) e^{\,m_{j} - m_{i+j}} $$  
$$ = \sum_{k=0}^{i+j} e^{\,x_k - m_{i+j}} $$





## Online-Self Attention
attention에는 softmax 때문에 input 전체를 봐야한다.

하지만, softmax는 Online-softmax로 전체를 안봐도 계산이 가능하다

self-attention에도 이를 적용한다.

Online-Self Attention을 실제로 알고리즘 관점에서 자세하게 분석한게 FlashAttention이다.


귀납법으로 증명



## Conclusion

