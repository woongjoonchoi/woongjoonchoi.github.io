---
title : "Efficient Attention"



excerpt: "Fused Kernel"

categories:
  - Kernel_Fusion
tags:
  - [GPU Programming,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---
## Introduction

Self-Attention은 Transformer 논문에서 처음 등장하여 현재까지 널리 쓰이고 있습니다.
Self-Attention은 병렬적으로 sequence의 embedding을 계산할 수 있는 점 때문에 한번에 계산하는 context를 키운 LLM으로 발전하게 됩니다.
LLM으로 발전하면서 모델의 실행 비용이 self-attention에 의존적으로 변합니다.따라서, self-attention을 효율적으로 구현하면 할 수록 모델의 실행 비용이 줄어들게 됩니다.
self-attention을 수학적으로 동일하지만 Memory Efficient하게 구현하는 방법이 주목을 받고 있습니다.


## BackGround



### Attention
Attention은 기존의 RNN,GRU,LSTM 스타일의 encoder-decoder[link](https://arxiv.org/pdf/1406.1078) 의 한계점을 보완하기 위해 나왔습니다.
Attention[[link]](https://arxiv.org/pdf/1409.0473)은 Dzmitry Bahdanau라는 분과 뉴옥대의 조경현 교수님께서 처음 제안하셨습니다.  

조경현 교수님께서는 fixed size vector에 sequence vector를 압축하는것이 불가능하다고 결론을 내리셨습니다.[[link]](https://www.boostcourse.org/ai331/lecture/540234?isDesc=false)  

![rnn-attention](\assets\images\DeepLearning\KernelFusion\rnn-attention.png)  
따라서, sequence vector를 fixed size로 압축해서  decoder의 input에 주지 않고 token length에  비례한 vector를 decoder에 input으로 주었습니다.
input sequence를 tokenize한 embedding vector들로 정보를 만들어야 합니다.
![vector-diff](\assets\images\DeepLearning\KernelFusion\vectordiff.png)  
![country-capital](\assets\images\DeepLearning\KernelFusion\country-capital.png) 

embedding 된 sequence간의 relation은 vector의 성질중 하나인 방향과 크기가 같다면 같다라는 성질이 만족함이 Thomas Mikolov의 paper[[link1]](https://arxiv.org/pdf/1310.4546),[[link2]](https://aclanthology.org/N13-1090.pdf)에 의해 보여졌습니다.
이는 embedding 들을 더하면 위치에 관계없이 어떤 의미를 나타내는 vector를 만들 수 있음을 의미합니다.
따라서, tokenize된 sequence는  embedding vector의 sequence라 볼 수 있고 이 embedding vector들을 더하면 어떠한 의미를 나타내게 됩니다. 
이 때, 어떤 output vector를 만들어 낼때 input embedding vector가 항상 동등하게 사용되는 것은 아닙니다.
따라서, decoder의 time step마다 어떤 embedding이 필요한지 계산하여 이 embedding vector와 input embedding vector의 유사도를 계산하여 필요한 vector를 weighted sum을 계산해줄려합니다.이때 , 유사도의 범위를 조정해주기 위해서 softmax function을 사용합니다.
따라서, 

![country-capital](\assets\images\DeepLearning\KernelFusion\attention-result.png)   
그 결과 , input sequence의 정보가 손실되지 않았습니다. 



#### Self Attention
Self-Attention은 기존의 Attention에서 RNN을 Fully-Connected Network로 대체한 것입니다.  
Fully-Connected Network를 사용함으로서 GEMM을 적용할 수 있게 되고, 따라서 병렬적으로 계산을 할 수 있게 됩니다. 
GEMM에 대한 각종 최적화를 적용할 수 있기에 RNN Attention에 비해 더 빠릅니다. 
Self-Attention은 처음 등장 이후 GPT-2,GPT-3 등의 다양한 LLM에 적용이 되는 동시에, VIT 같은 vision modality에도 적용이 되었습니다. 최근에는, Langauge, Vision 등의 다양한 modality를 self-attention 이 처리하는 Large Multi-Modal Model의 핵심적인 모듈로서 역할을 하고 있습니다.

### Efficient Attention

Efficient Attention의 일반적인 접근 방법은 computation Complexity의 Upper Bound를 낮추는 방법입니다. 
context를 전체 input이 아닌 부분으로 한정하는 Window Attention 이 있습니다.Linear Attention은 Q@K^T를 계산할 때 (n,d) ,(d,n) 의 matrix들을 곱해서 계산하는게 아닌 K^T @V를 먼저 계산함. 즉, (d,n) @ (n,d) 의 matrix multiplication을 통해서 computation complexity를 O(N)으로 낮춥니다.
하지만, 이러한 efficient attention은 computation Complexity의 장점을 얻는 대신에 정확도에서 단점을 얻게 됩니다.
따라서, Memory Efficient한 Attention이 주목을 받고 있습니다. Gpu Programming을 활용하여 정확도를 유지한채 연산속도와 메모리 사용량을 상당부분 향상시키기 때문입니다.

## Softmax
Attention은 embedding vector들을 합하지만 모든 embedding vector를 동일하게 사용하는 것이 아니기에 가중치를 계산하게 됩니다. 이 때, 사용하는 것이 softmax입니다.
softmax는 받은 vector의 element의 합이 1이 되도록 , 각 element가 0이 되도록 변환을 해줍니다.
### Safe-Softmax

$$ y_i = \frac{e^{x_i}}{\sum_{j=1}^V e^{x_j}} $$
기존의 softmax는 exponential 함수를 사용하기에 overflow나 underflow 문제가 발생할 수 있다.

따라서,max값을 빼주어서 overflow나 underflow를 방지한다.
$$ y_i = \frac{e^{x_i - \max_{k=1}^V x_k}}{\sum_{j=1}^V e^{x_j - \max_{k=1}^V x_k}} $$


![online-softmax](\assets\images\DeepLearning\KernelFusion\safesoftmax.png)



### Online-Softmax 
하지만,softmax는 전체 row를 봐야한다.
전체 row를 메모리에 매번 적재를 하는 것은 부담이 됩니다.
따라서, input의 일부만을 확인하고 매번 분모와 분자를 update한다.

![online-softmax](\assets\images\DeepLearning\KernelFusion\online-softmax.png)


#### Proof 

**Base case**: \(V = 1\)

1. $$m_1 \leftarrow x_1$$  
   $$= \max_{k=1}^1 x_k$$

2. $$d_1 \leftarrow e^{x_1 - m_1}$$  
   $$= \sum_{j=1}^1 e^{x_j - m_1}$$


**Induction**: \(V = S\)

1. $$ Assume \quad m_{S-1} = \max_{k=1}^{S-1} x_k $$  
$$m_S \leftarrow \max(m_{S-1}, x_S) $$  
$$= \max\bigl(\max_{k=1}^{S-1} x_k,\; x_S\bigr) $$  
$$ = \max_{k=1}^{S} x_k $$  



2. $$ Assume \quad d_{S-1} = \sum_{j=1}^{S-1} e^{\,x_j - m_S} $$  
$$d_S \leftarrow d_{S-1} \, e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$= \left(\sum_{j=1}^{S-1} e^{\,x_j - m_{S-1}}\right) e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$ = \sum_{j=1}^{S} e^{\,x_j - m_S} $$

#### SafeNess 

- $$m_j \in \Bigl[\min_{k=1}^V m_k,\; \max_{k=1}^V m_k\Bigr], \quad \forall j \in \{1, \dots, V\}$$  
  따라서,, $$m_j$$ 는 overflow나 underflow가 될 수 없습니다.

- $$1 \;\le\; d_j \;\le\; j, \quad \forall j \in \{1, \dots, V\}$$  
  $$d_j $$의 식은 다음과 같습니다.  
  $$d_S \leftarrow d_{j-1} \, e^{\,m_{j-1} - m_j} + e^{\,x_j - m_j} $$  
  매번 1보다 같거나 작은 수를 $$d_j $$에 곱하고 $$d_j $$에 더해줍니다.  
  따라서, 32-bit floating point 는 $$d_j$$가 $$1.7 \times 10^{37}$$ 까지 저장할 수 있도록 보장해줍니다.
  왜냐하면, 32-bit floating point 는 $$ 3.4 \times 10^{38}$$ 까지 표현할 수 있기 때문입니다. 
  32-bit floating point의 range보다 적은 값까지 표현이 가능하다 하는 이유는 안전수치를 보수적으로 잡은거 같습니다. 
  만약에, 더 많은 벡터를 처리한다면 64-bit floating point 를 사용해야 할 것입니다.  
  
#### Block-Update Proof
위에서 증명한 것들은 Online-Softmax를 1개의 data를 차례대로 계산하여 update하는 과정을 증명한 것입니다.  
실제로, data를 여러개 받아서 $$d_j $$ , $$m_j $$ 등을 계산하기 때문에 이를 기존에 구한 $$d_i $$ , $$m_i $$ 에 반영할 수 있어야 합니다.  

$$
\begin{bmatrix}
m_i \\
d_i
\end{bmatrix}
\;\oplus\;
\begin{bmatrix}
m_j \\
d_j
\end{bmatrix}
=
\begin{bmatrix}
\max(m_i, m_j) \\
d_i \, e^{\,m_i - \max(m_i, m_j)} \;+\; d_j \, e^{\,m_j - \max(m_i, m_j)}
\end{bmatrix}
$$

$$d_i $$ , $$m_i $$ 를 미리 구해놓고 다음 input에 대하여 $$d_j $$ , $$m_j $$ 를 계산하면 위의 공식에 대입하여 update를 할 수 있습니다.  
max에 대한 증명은 생략하도록 하겠습니다. 

1. $$ Assume \quad d_{j} = \sum_{k=0}^{j} e^{\,x_k - m_i} \quad d_{i} = \sum_{k=i+1}^{j+i} e^{\,x_k - m_j} \quad m_{i+j} = \max(m_i, m_j) $$  
$$d_i \oplus d_{j} = d_i \, e^{\,m_i - \max(m_i, m_j)} \;+\; d_j \, e^{\,m_j - \max(m_i, m_j)} $$  
$$= \left(\sum_{k=0}^{i} e^{\,x_k - m_{i}}\right) e^{\,m_{i} - \max(m_i, m_j)} + \left(\sum_{k=i+1}^{i+j} e^{\,x_k - m_{i}}\right) e^{\,m_{j} - \max(m_i, m_j)} $$  
$$= \left(\sum_{k=0}^{i} e^{\,x_k - m_{i}}\right) e^{\,m_{i} - m_{i+j}} + \left(\sum_{k=i+1}^{i+j} e^{\,x_k - m_{i}}\right) e^{\,m_{j} - m_{i+j}} $$  
$$ = \sum_{k=0}^{i+j} e^{\,x_k - m_{i+j}} $$





## Online-Self Attention
attention에는 softmax 때문에 input 전체를 봐야한다.

하지만, softmax는 Online-softmax로 전체를 안봐도 계산이 가능하다

self-attention에도 이를 적용한다.[[link]](https://arxiv.org/pdf/2112.05682) 
$$ v^* \in \mathbb{R}^d $$ ,$$ s^* \in \mathbb{R} $$ 를 0으로 초기화하고 , $$m^*$$를 -inf로 초기화합니다.
query $$q$$, keys $$k_1, \dots, k_n$$ 와 values $$v_1, \dots, v_n$$ 가 주어질 때 , keys와 values들을 순서대로 사용합니다.$$k_i$$ , $$v_i$$ 가 주어지면 $$ s_i = \mathrm{dot}(q, k_i) $$ 를 계산합니다.그리고 나서,$$m^* = \max(m^*,s_i)$$, $$ v^* \leftarrow v^* e^{m^* - m_i} + v_i e^{s_i - m_i} $$ , $$ s^* \leftarrow s^* e^{m^* - m_i} + e^{s_i - m_i} $$ 를  update합니다.그리고,$$ \frac{v^*}{s^*} $$를 계산해줍니다.



### Proof 

$$m^* = \max(m^*,s_i)$$, ,$$ s^* \leftarrow s^* e^{m^* - m_i} + e^{s_i - m_i} $$ 들은 위에서 증명이 되었습니다.  
따라서, 증명을 생략하도록 하겠습니다. $$ v^* \leftarrow v^* e^{m^* - m_i} + v_i e^{s_i - m_i} $$에 대해서 증명을 하도록 하겠습니다.  

**Base case**: \(V = 1\)  

1. $$v^* \leftarrow v^* e^{m^* - m_1} + v_1 e^{s_1 - m_1} $$  
   $$= v_1 e^{s_1 - m_1}$$


**Induction**: \(V = i\)  



1. $$ Assume \quad v^* = \sum_{j=1}^{i-1} v_je^{\,x_j - m^*} $$  
$$ m_i = \max(m^*,s_i)$$ , $$ s_i = \mathrm{dot}(q, k_i) $$  
$$ v^* \leftarrow v^* e^{m^* - m_i} + v_i e^{s_i - m_i} $$  
$$= \left(\sum_{j=1}^{i-1} v_j e^{\,x_j - m^*}\right) e^{\,m^* - m_i} + v_i e^{\,s_i - m_i} $$  
$$ = \sum_{j=1}^{i} v_j  e^{\,s_j - m_i} $$  
그리고, $$ \frac{v^*}{s^*} $$ 를 계산해줍니다.  
$$m_i = \max(m^*,s_i)$$ 가 현재까지 계산한 $$ \max $$ 이기 때문에 online self-attention 수식이 정확함이 증명이 됩니다.


## FlashAttention



Online-Self Attention을 실제로 구현체를 Open-source로 제공한 것이 FlashAttention이다.
FlashAttention은 Tri Dao라는 사람이 제안을 하였습니다.FlashAttention의 알고리즘에서 idea가 새로 나온것은 없지만 Online-Self Attention을 실제로 여러 LLM에 적용할 수 있도록 구현체를 제공한 최초의 사례입니다. 

### Forward
![FlashAttention](\assets\images\DeepLearning\KernelFusion\FlashAttention_forward.png)  

FlashAttention의 forward는 이와 같습니다. algorithm의 line벼로 설명을 하도록 하겠습니다.

1. 

2. 위의 online-self attention 에서 보았듯이 $$ v^* \in \mathbb{R}^d $$ ,$$ s^* \in \mathbb{R} $$ 를 0으로 초기화 했는데, FlashAttention에서는 이를 $$ O , \ell $$ 로 각각 표기법을 바꾸었습니다. Batch dimension으로 확장했기에 $$ O \in \mathbb{R}^{N \times d} $$ ,$$ \ell \in \mathbb{R}^N $$ ,$$m = (-\infty)_N \in \mathbb{R}^N $$ 로 초기화합니다.




#### Backward


## Conclusion



## References