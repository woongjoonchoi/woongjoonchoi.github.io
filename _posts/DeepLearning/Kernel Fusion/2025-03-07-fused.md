---
title : "Efficient Attention"



excerpt: "Fused Kernel"

categories:
  - Kernel_Fusion
tags:
  - [GPU Programming,deep learning ]
# classes : wide
toc: true
toc_sticky: true
---
## Introduction

Self-Attention은 Transformer 논문에서 처음 등장하여 현재까지 널리 쓰이고 있습니다.
Self-Attention은 병렬적으로 sequence의 embedding을 계산할 수 있는 점 때문에 한번에 계산하는 context를 키운 LLM으로 발전하게 됩니다.
LLM으로 발전하면서 모델의 실행 비용이 self-attention에 의존적으로 변합니다.따라서, self-attention을 효율적으로 구현하면 할 수록 모델의 실행 비용이 줄어들게 됩니다.
self-attention을 수학적으로 동일하지만 Memory Efficient하게 구현하는 방법이 주목을 받고 있습니다.


## BackGround

### Self-Attention


### Efficient Attention

Efficient Attention의 일반적인 접근 방법은 computation Complexity의 Upper Bound를 낮추는 방법입니다. 
context를 전체 input이 아닌 부분으로 한정하는 Window Attention 이 있습니다.Linear Attention은 Q@K^T를 계산할 때 (n,d) ,(d,n) 의 matrix들을 곱해서 계산하는게 아닌 K^T @V를 먼저 계산함. 즉, (d,n) @ (n,d) 의 matrix multiplication을 통해서 computation complexity를 O(N)으로 낮춥니다.
하지만, 이러한 efficient attention은 computation Complexity의 장점을 얻는 대신에 정확도에서 단점을 얻게 됩니다.
따라서, Memory Efficient한 Attention이 주목을 받고 있습니다. Gpu Programming을 활용하여 정확도를 유지한채 연산속도와 메모리 사용량을 상당부분 향상시키기 때문입니다.

## Softmax

### Safe-Softmax

$$ y_i = \frac{e^{x_i}}{\sum_{j=1}^V e^{x_j}} $$
기존의 softmax는 exponential 함수를 사용하기에 overflow나 underflow 문제가 발생할 수 있다.

따라서,max값을 빼주어서 overflow나 underflow를 방지한다.
$$ y_i = \frac{e^{x_i - \max_{k=1}^V x_k}}{\sum_{j=1}^V e^{x_j - \max_{k=1}^V x_k}} $$


![online-softmax](\assets\images\DeepLearning\KernelFusion\safesoftmax.png)



### Online-Softmax 
하지만,softmax는 전체 row를 봐야한다.
전체 row를 메모리에 매번 적재를 하는 것은 부담이 됩니다.
따라서, input의 일부만을 확인하고 매번 분모와 분자를 update한다.

![online-softmax](\assets\images\DeepLearning\KernelFusion\online-softmax.png)


#### Proof 

**Base case**: \(V = 1\)

1. $$m_1 \leftarrow x_1$$  
   $$= \max_{k=1}^1 x_k$$

2. $$d_1 \leftarrow e^{x_1 - m_1}$$  
   $$= \sum_{j=1}^1 e^{x_j - m_1}$$


**Induction**: \(V = S\)

1. $$ Assume \quad m_{S-1} = \max_{k=1}^{S-1} x_k $$  
$$m_S \leftarrow \max(m_{S-1}, x_S) $$  
$$= \max\bigl(\max_{k=1}^{S-1} x_k,\; x_S\bigr) $$  
$$ = \max_{k=1}^{S} x_k $$  



2. $$ Assume \quad d_{S-1} = \sum_{j=1}^{S-1} e^{\,x_j - m_S} $$  
$$d_S \leftarrow d_{S-1} \, e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$= \left(\sum_{j=1}^{S-1} e^{\,x_j - m_{S-1}}\right) e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$ = \sum_{j=1}^{S} e^{\,x_j - m_S} $$

#### SafeNess 

**SafeNess** :  
- $$m_j \in \Bigl[\min_{k=1}^V m_k,\; \max_{k=1}^V m_k\Bigr], \quad \forall j \in \{1, \dots, V\}$$  
  따라서,, $$m_j$$ 는 overflow나 underflow가 될 수 없습니다.

- $$1 \;\le\; d_j \;\le\; j, \quad \forall j \in \{1, \dots, V\}$$  
  $$d_j $$의 식은 다음과 같습니다.  
  $$d_S \leftarrow d_{j-1} \, e^{\,m_{j-1} - m_j} + e^{\,x_j - m_j} $$  
  매번 1보다 같거나 작은 수를 $$d_j $$에 곱하고 $$d_j $$에 더해줍니다.  
  따라서, 32-bit floating point 는 $$d_j$$가 $$1.7 \times 10^{37}$$ 까지 저장할 수 있도록 보장해줍니다.
  왜냐하면, 32-bit floating point 는 $$ 3.4 \times 10^{38}$$ 까지 표현할 수 있기 때문입니다. 
  32-bit floating point의 range보다 적은 값까지 표현이 가능하다 하는 이유는 안전수치를 보수적으로 잡은거 같습니다. 
  만약에, 더 많은 벡터를 처리한다면 64-bit floating point 를 사용해야 할 것입니다.  
  
#### Block-Update Proof
위에서 증명한 것들은 Online-Softmax를 1개의 data를 차례대로 계산하여 update하는 과정을 증명한 것입니다.  
실제로, data를 여러개 받아서 $$d_j $$ , $$m_j $$ 등을 계산하기 때문에 이를 기존에 구한 $$d_i $$ , $$m_i $$ 에 반영할 수 있어야 합니다.  

$$
\begin{bmatrix}
m_i \\
d_i
\end{bmatrix}
\;\oplus\;
\begin{bmatrix}
m_j \\
d_j
\end{bmatrix}
=
\begin{bmatrix}
\max(m_i, m_j) \\
d_i \, e^{\,m_i - \max(m_i, m_j)} \;+\; d_j \, e^{\,m_j - \max(m_i, m_j)}
\end{bmatrix}
$$

$$d_i $$ , $$m_i $$ 를 미리 구해놓고 다음 input에 대하여 $$d_j $$ , $$m_j $$ 를 계산하면 위의 공식에 대입하여 update를 할 수 있습니다.
max에 대한 증명은 생략하도록 하겠습니다. 

1. $$ Assume \quad d_{j} = \sum_^{j} e^{\,x_k - m_S} $$  
$$d_S \leftarrow d_{S-1} \, e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$= \left(\sum_{j=1}^{S-1} e^{\,x_j - m_{S-1}}\right) e^{\,m_{S-1} - m_S} + e^{\,x_S - m_S} $$  
$$ = \sum_{j=1}^{S} e^{\,x_j - m_S} $$


## Online-Self Attention
attention에는 softmax 때문에 input 전체를 봐야한다.

하지만, softmax는 Online-softmax로 전체를 안봐도 계산이 가능하다

self-attention에도 이를 적용한다.

Online-Self Attention을 실제로 알고리즘 관점에서 자세하게 분석한게 FlashAttention이다.


귀납법으로 증명



## Conclusion

