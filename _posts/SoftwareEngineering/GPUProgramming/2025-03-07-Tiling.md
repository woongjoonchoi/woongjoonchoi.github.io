---
title : "Efficient GEMM"

excerpt: "Tiling"

categories:
  - GPU_Programming
tags:
  - [Software Engineering , GPU Programming]
# classes : wide
toc: true
toc_sticky: true
---

## Introduction
Matrix Multiplication은 다양한 과학 분야에서 널리 사용된다. Matrix Multiplication은 본질적으로 단순한 산술 연산의 집합으로 구성되어 있기 때문에, 상대적으로 더 많은 수의 코어(core)를 갖춘 GPU를 이용하여 효율적으로 수행된다. 그러나 GPU를 활용하여 Matrix Multiplication을 수행할 때, 빈번하게 발생하는 Memory Hierarchy 간 데이터 이동 문제가 성능 저하의 주요 원인이 된다.

일반적으로 Matrix Multiplication은 다음과 같이 정의된다.

$$
c_{ij} = \sum_{k} a_{ik} \times b_{kj}
$$

한 변의 길이가 $$O(N)$$인 행렬의 계산(computation) 복잡도는 $$O(N^3)$$이며, 계산을 수행하기 위해 GPU의 core에 로드되어야 하는 데이터의 크기는 $$O(N^2)$$이다. 데이터를 최소한으로 로드하면서 효율성을 극대화하기 위해서는 데이터가 한 번 로드될 때마다 $$O(N)$$의 연산(computation)을 수행해야 한다. 그러나 실제로 $$O(N^2)$$ 크기의 데이터는 일반적으로 매우 크기 때문에 모든 데이터를 한 번에 캐시(cache)에 로드하는 것이 불가능하다. 따라서 연산량은 동일하지만, 데이터를 여러 번 반복적으로 로드해야 하므로, 이러한 데이터의 빈번한 로드 과정이 병목 현상(bottleneck)의 주요 원인이 된다.

모든 $$N$$에 대해 앞서 정의한 일반적인 형태인 $$c_{ij} = \sum_{k} a_{ik} \times b_{kj}$$의 계산 방식을 그대로 수행한다면, 빈번한 데이터 로드 문제는 해결할 수 없다. 이 문제를 해결하기 위해, Matrix Multiplication의 정의를 수학적으로 동일하지만 다르게 해석하여 계산한다. 즉, Matrix Multiplication을 Block Matrix Multiplication의 형태로 재정의하여 계산하는 것이다. Block Matrix Multiplication을 활용하여 Matrix Multiplication을 수행하면 데이터 로드 횟수를 줄일 수 있다.

더 나아가, 각 Block 내에서도 Block Matrix Multiplication을 다시 수행하도록 분해하여 구현할 수 있다. 추가적으로, Block Multiplication 내에서도 Outer Product 형태로 정의하여 구현함으로써, 메모리 접근 효율성을 더욱 향상할 수 있다.

## BackGround

### Memory Hierarchy

![memory-hierarch](\assets\images\software_engineering\gpu_programming\memory-hierarchy-in-gpus-1-625x381.png)
![flashattention](\assets\images\software_engineering\gpu_programming\flashattention.png)


GPU의 **Memory Hierarchy**는 **DRAM, L2 Cache, L1 Cache/Shared Memory, Register**로 구분됩니다.  
GPU가 연산을 수행하기 위해 데이터에 접근할 때, **일반적으로 DRAM에서 데이터를 가져와 L2 Cache, L1 Cache/Shared Memory를 거쳐 Register에 저장한 후 연산을 수행**합니다.  
또한, **L1 Cache/Shared Memory로 직접 데이터를 로드하는 경우에는 L2 Cache를 거치지 않고 곧바로 L1 Cache/Shared Memory로 전달된 후 Register로 이동합니다.  

GPU는 **Streaming Multiprocessor (SM)** 라는 연산 유닛을 포함하고 있으며, **프로그램이 실행될 때 SM 내에서 PTX라는 Instruction Set Architecture (ISA)** 가 동작합니다. **PTX 명령어는 opcode와 operand로 구성되며**, opcode는 사전에 정의된 명령어 코드들이고, **operand는 주로 Register에 저장된 값들**을 사용합니다. 따라서, **GPU가 실제로 연산을 수행하려면 필요한 데이터가 DRAM으로부터 최종적으로 Register까지 전송되어야 합니다.**

GPU에서 **Register는 가장 빠른 저장 장치이며, Register에 물리적으로 가까운 저장 장치일수록 데이터 전송 속도(bandwidth)가 빠릅니다.**  
최근의 최신 GPU의 경우 **DRAM 용량은 최대 80GB**, **L2 Cache는 약 50MB**, **L1 Cache/Shared Memory는 최대 약 33MB** 수준입니다.  

특히 **딥러닝**과 같은 대규모 데이터 연산에서는 **행렬(Matrix) 전체를 캐시에 로드할 수 없는 경우가 빈번히 발생**하기 때문에, **여러 번 DRAM에 접근하여 필요한 데이터를 L2, L1/Shared Memory, Register로 가져오는 작업이 반복**됩니다.  
하지만 **DRAM의 접근 속도와 대역폭은 L2 Cache, L1 Cache/Shared Memory, Register에 비해 매우 느리기 때문에**, **DRAM에서 Register로의 데이터 전송을 최소화하는 것이 GPU 연산 최적화의 핵심**입니다.

### Arthimetic Intensity
Register로의 데이터 전송을 최적화하게 되면 **Arithmetic Intensity**가 증가하게 됩니다.  
**Arithmetic Intensity**는 다음과 같이 정의됩니다.

$$
\text{Arithmetic Intensity} = \frac{\text{Arithmetic Operation Work}}{\text{Transferred Bytes per second}}
$$  

GPU에서 연산을 최적화하기 위해서는 **DRAM에서 Register로 전송되는 데이터의 양을 최소화하는 것이 중요합니다.**  
이는 앞서 언급한 것처럼, **Matrix Multiplication과 같은 연산을 고려할 때 전체 계산량(Arithmetic Operation Work)은 고정되어 있기 때문입니다.**

특히, **행렬(Matrix)의 각 요소(element)가 연산에 사용되는 횟수는 결정되어 있기 때문에**, **동일한 데이터를 DRAM에서 여러 번 가져오는 것을 줄이면 데이터 전송 시간이 감소하고**, 결과적으로 **Arithmetic Intensity가 증가**하게 됩니다.

이러한 이유로 **Arithmetic Intensity는 딥러닝 연산이 얼마나 효율적으로 수행되는지를 나타내는 중요한 지표**로 사용됩니다.  
또한, **Arithmetic Intensity가 높다는 것은 동일한 데이터를 메모리로부터 로드했을 때 수행되는 연산의 양이 많다는 의미이며**, 이는 **메모리 대역폭 대비 계산량이 많은 구조로, 메모리 병목을 줄이고 GPU의 계산 성능을 극대화할 수 있음을 의미합니다.**


### Matrix Mutliplication Definition  

**Matrix Multiplication**은 다음과 같이 정의됩니다.

$$
c_{ij} = \sum_{k} a_{ik} \times b_{kj}
$$

하지만, 이러한 정의를 사용하면 **Matrix Multiplication은 중첩된 for loop**으로 구현됩니다.

```
for k K
  for i  M
    for j N
      c[i][j] += A[i][k] *B[k][j]
```

이때 **C의 한 개의 원소**를 계산하기 위해서는 **A의 하나의 row와 B의 하나의 column 전체를 load**해야 합니다.  
따라서, **일반적인 Matrix Multiplication 정의를 그대로 사용하게 되면**, **A와 B 행렬의 각 원소를 C[i][j]를 계산하는 순간마다 메모리에서 가져와서 Register로 로드한 후 연산**을 수행하게 됩니다.

결과적으로, **하나의 C[i][j]를 계산하기 위해 A와 B의 데이터를 여러 번 반복해서 load**하게 되며, **A와 B의 각 원소가 O(N)번씩 메모리로부터 로드**되는 비효율이 발생합니다.

#### Block Matrix Multiplication
Matrix는 **여러 개의 작은 Matrix Block으로 나누어 표현**될 수 있습니다.  
그리고 **모든 Matrix Multiplication은 이러한 Matrix Block들의 곱으로 표현할 수 있습니다.**  
즉, 큰 행렬의 곱셈도 작은 블록 단위로 나누어 계산할 수 있습니다.

예를 들어, 다음과 같이 두 행렬을 Block 형태로 나눌 수 있습니다.

$$
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}
\begin{bmatrix}
B_{11} & B_{21}
\end{bmatrix}
=
\begin{bmatrix}
A_{11} B_{11} + A_{12} B_{21} \\
A_{21} B_{11} + A_{22} B_{21}
\end{bmatrix}
$$

따라서, **Matrix Multiplication은 Block 단위의 곱셈과 덧셈으로 분할하여 표현**할 수 있으며, 이는 대규모 행렬 연산의 효율성을 높이기 위해 널리 사용되는 방식입니다.



#### Outer Product


**Outer Product**란 어떤 벡터 $v \ (m \times 1)$, $u \ (n \times 1)$ 가 있을 때,  
$v$와 $u^T$의 외적(Outer Product)을 통해 $(m \times n)$ 크기의 행렬을 만들어내는 계산 방법입니다.

또한, **어떤 Matrix Multiplication도 여러 개의 Outer Product의 합으로 표현할 수 있습니다.**  
즉, 다음과 같이 나타낼 수 있습니다.

$$
AB = \sum_k (a_{\text{col}_k} \otimes b_{\text{row}_k})
$$

여기서 $a_{\text{col}_k}$는 A의 열(column), $b_{\text{row}_k}$는 B의 행(row)을 의미하며, $\otimes$는 Outer Product를 의미합니다.

이 방법의 장점은 **각 행렬의 원소(element)에 대해 외적(Outer Product)을 한 번 수행한 이후에는 해당 element가 다시 필요 없다는 점**입니다.  
즉, **한 번 로드한 데이터를 재사용하지 않고 곧바로 모든 연산에 사용**할 수 있어 메모리 접근을 최소화할 수 있습니다.

구체적으로, **Register에 1차원 배열로 row 또는 column을 로드**한 후, **한 번의 Outer Product 연산**을 수행하면 **이미 Register에 로드된 element들을 다시 메모리에서 로드할 필요가 없습니다.**  
따라서, **효율적인 데이터 사용과 최소한의 메모리 접근**이 가능해지며, 이는 특히 GPU에서 연산 최적화를 위해 매우 중요한 기법입니다.


## Tiling
하지만, A와 B 전체를 cache에 load할 수 없습니다. (4000,4000) 정도의 shape의 matrix가 fp16의 data type이라면 cache size를 초과합니다.

### Block Tiling

```

for bm M
  for bn N
    for bk K
      A block load
      B block load
      C block load
      for i bm
        for j bn
          c[i][j] += A[i][k] *B[k][j]

```



### Warp Tiling


### Thread Tiling

## Conclusion


## References

https://docs.nvidia.com/cuda/parallel-thread-execution/  

https://product.kyobobook.co.kr/detail/S000003500906